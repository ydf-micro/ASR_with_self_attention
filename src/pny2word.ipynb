{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "['lv4', 'shi4', 'yang2', 'chun1', 'yan1', 'jing3', 'da4', 'kuai4', 'wen2', 'zhang1', 'de', 'di3', 'se4', 'si4', 'yue4', 'de', 'lin2', 'luan2', 'geng4', 'shi4', 'lv4', 'de2', 'xian1', 'huo2', 'xiu4', 'mei4', 'shi1', 'yi4', 'ang4', 'ran2']\n['绿', '是', '阳', '春', '烟', '景', '大', '块', '文', '章', '的', '底', '色', '四', '月', '的', '林', '峦', '更', '是', '绿', '得', '鲜', '活', '秀', '媚', '诗', '意', '盎', '然']\n\n['ta1', 'jin3', 'ping2', 'yao1', 'bu4', 'de', 'li4', 'liang4', 'zai4', 'yong3', 'dao4', 'shang4', 'xia4', 'fan1', 'teng2', 'yong3', 'dong4', 'she2', 'xing2', 'zhuang4', 'ru2', 'hai3', 'tun2', 'yi1', 'zhi2', 'yi3', 'yi1', 'tou2', 'de', 'you1', 'shi4', 'ling3', 'xian1']\n['他', '仅', '凭', '腰', '部', '的', '力', '量', '在', '泳', '道', '上', '下', '翻', '腾', '蛹', '动', '蛇', '行', '状', '如', '海', '豚', '一', '直', '以', '一', '头', '的', '优', '势', '领', '先']\n\n['pao4', 'yan3', 'da3', 'hao3', 'le', 'zha4', 'yao4', 'zen3', 'me', 'zhuang1', 'yue4', 'zheng4', 'cai2', 'yao3', 'le', 'yao3', 'ya2', 'shu1', 'de', 'tuo1', 'qu4', 'yi1', 'fu2', 'guang1', 'bang3', 'zi', 'chong1', 'jin4', 'le', 'shui3', 'cuan4', 'dong4']\n['炮', '眼', '打', '好', '了', '炸', '药', '怎', '么', '装', '岳', '正', '才', '咬', '了', '咬', '牙', '倏', '地', '脱', '去', '衣', '服', '光', '膀', '子', '冲', '进', '了', '水', '窜', '洞']\n\n['ke3', 'shei2', 'zhi1', 'wen2', 'wan2', 'hou4', 'ta1', 'yi1', 'zhao4', 'jing4', 'zi', 'zhi3', 'jian4', 'zuo3', 'xia4', 'yan3', 'jian3', 'de', 'xian4', 'you4', 'cu1', 'you4', 'hei1', 'yu3', 'you4', 'ce4', 'ming2', 'xian3', 'bu4', 'dui4', 'cheng1']\n['可', '谁', '知', '纹', '完', '后', '她', '一', '照', '镜', '子', '只', '见', '左', '下', '眼', '睑', '的', '线', '又', '粗', '又', '黑', '与', '右', '侧', '明', '显', '不', '对', '称']\n\n['qi1', 'shi2', 'nian2', 'dai4', 'mo4', 'wo3', 'wai4', 'chu1', 'qiu2', 'xue2', 'mu3', 'qin1', 'ding1', 'ning2', 'wo3', 'chi1', 'fan4', 'yao4', 'xi4', 'jue2', 'man4', 'yan4', 'xue2', 'xi2', 'yao4', 'shen1', 'zuan1', 'xi4', 'yan2']\n['七', '十', '年', '代', '末', '我', '外', '出', '求', '学', '母', '亲', '叮', '咛', '我', '吃', '饭', '要', '细', '嚼', '慢', '咽', '学', '习', '要', '深', '钻', '细', '研']\n\n['yi1', 'jin4', 'men2', 'wo3', 'bei4', 'jing1', 'dai1', 'le', 'zhe4', 'hu4', 'ming2', 'jiao4', 'pang2', 'ji2', 'de', 'lao3', 'nong2', 'shi4', 'kang4', 'mei3', 'yuan2', 'chao2', 'fu4', 'shang1', 'hui2', 'xiang1', 'de', 'lao3', 'bing1', 'qi1', 'zi', 'zhang3', 'nian2', 'you3', 'bing4', 'jia1', 'tu2', 'si4', 'bi4', 'yi1', 'pin2', 'ru2', 'xi3']\n['一', '进', '门', '我', '被', '惊', '呆', '了', '这', '户', '名', '叫', '庞', '吉', '的', '老', '农', '是', '抗', '美', '援', '朝', '负', '伤', '回', '乡', '的', '老', '兵', '妻', '子', '长', '年', '有', '病', '家', '徒', '四', '壁', '一', '贫', '如', '洗']\n\n['zou3', 'chu1', 'cun1', 'zi', 'lao3', 'yuan3', 'lao3', 'yuan3', 'wo3', 'hai2', 'hui2', 'tou2', 'zhang1', 'wang4', 'na4', 'ge4', 'an1', 'ning2', 'tian2', 'jing4', 'de', 'xiao3', 'yuan4', 'na4', 'ge4', 'shi3', 'wo3', 'zhong1', 'shen1', 'nan2', 'wang4', 'de', 'xiao3', 'yuan4']\n['走', '出', '村', '子', '老', '远', '老', '远', '我', '还', '回', '头', '张', '望', '那', '个', '安', '宁', '恬', '静', '的', '小', '院', '那', '个', '使', '我', '终', '身', '难', '忘', '的', '小', '院']\n\n['er4', 'yue4', 'si4', 'ri4', 'zhu4', 'jin4', 'xin1', 'xi1', 'men2', 'wai4', 'luo2', 'jia1', 'nian3', 'wang2', 'jia1', 'gang1', 'zhu1', 'zi4', 'qing1', 'wen2', 'xun4', 'te4', 'de', 'cong2', 'dong1', 'men2', 'wai4', 'gan3', 'lai2', 'qing4', 'he4']\n['二', '月', '四', '日', '住', '进', '新', '西', '门', '外', '罗', '家', '碾', '王', '家', '冈', '朱', '自', '清', '闻', '讯', '特', '地', '从', '东', '门', '外', '赶', '来', '庆', '贺']\n\n['dan1', 'wei4', 'bu4', 'shi4', 'wo3', 'lao3', 'die1', 'kai1', 'de', 'ping2', 'shen2', 'me', 'yao4', 'yi1', 'ci4', 'er4', 'ci4', 'zhao4', 'gu4', 'wo3', 'wo3', 'bu4', 'neng2', 'ba3', 'zi4', 'ji3', 'de', 'bao1', 'fu2', 'wang3', 'xue2', 'xiao4', 'shuai3']\n['单', '位', '不', '是', '我', '老', '爹', '开', '的', '凭', '什', '么', '要', '一', '次', '二', '次', '照', '顾', '我', '我', '不', '能', '把', '自', '己', '的', '包', '袱', '往', '学', '校', '甩']\n\n['dou1', 'yong4', 'cao3', 'mao4', 'huo4', 'ge1', 'bo2', 'zhou3', 'hu4', 'zhe', 'wan3', 'lie4', 'lie4', 'ju1', 'ju1', 'chuan1', 'guo4', 'lan4', 'ni2', 'tang2', 'ban1', 'de', 'yuan4', 'ba4', 'pao3', 'hui2', 'zi4', 'ji3', 'de', 'su4', 'she3', 'qu4', 'le']\n['都', '用', '草', '帽', '或', '胳', '膊', '肘', '护', '着', '碗', '趔', '趔', '趄', '趄', '穿', '过', '烂', '泥', '塘', '般', '的', '院', '坝', '跑', '回', '自', '己', '的', '宿', '舍', '去', '了']\n\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "with open('../data/zh.tsv', 'r', encoding='utf-8') as f:\n",
    "    data = f.readlines()\n",
    "\n",
    "    inputs = []\n",
    "    labels = []\n",
    "    for i in range(len(data)):\n",
    "        key, pny, word = data[i].split('\\t')\n",
    "        inputs.append(pny.split(' '))\n",
    "        labels.append(word.strip('\\n').split(' '))\n",
    "    \n",
    "    for i in range(10):\n",
    "        print(inputs[i])\n",
    "        print(labels[i])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "拼音词表长度为: 1152\n文字词表长度为: 4460\n['<PAD>', 'lv4', 'shi4', 'yang2', 'chun1', 'yan1', 'jing3', 'da4', 'kuai4', 'wen2']\n['<PAD>', '绿', '是', '阳', '春', '烟', '景', '大', '块', '文']\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "def get_vocab(data):\n",
    "    vocab = ['<PAD>']\n",
    "    for line in data:\n",
    "        for char in line:\n",
    "            if char not in vocab:\n",
    "                vocab.append(char)\n",
    "                \n",
    "    return vocab\n",
    "\n",
    "pny2id = get_vocab(inputs)\n",
    "word2id = get_vocab(labels)\n",
    "\n",
    "print(f'拼音词表长度为: {len(pny2id)}')\n",
    "print(f'文字词表长度为: {len(word2id)}')\n",
    "\n",
    "print(pny2id[:10])\n",
    "print(word2id[:10])"
   ],
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "input_num = [[pny2id.index(pny) for pny in line] for line in inputs]\n",
    "label_num = [[word2id.index(word) for word in line] for line in labels]"
   ],
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "[[  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  11  16  17\n   18   2   1  19  20  21  22  23  24  25  26  27   0   0   0   0   0   0\n    0   0   0   0   0   0   0]\n [ 28  29  30  31  32  11  33  34  35  36  37  38  39  40  41  36  42  43\n   44  45  46  47  48  49  50  51  49  52  11  53   2  54  20   0   0   0\n    0   0   0   0   0   0   0]\n [ 55  56  57  58  59  60  61  62  63  64  15  65  66  67  59  67  68  69\n   11  70  71  49  72  73  74  75  76  77  59  78  79  42   0   0   0   0\n    0   0   0   0   0   0   0]\n [ 80  81  82   9  83  84  28  49  85  86  75  87  88  89  39  56  90  11\n   91  92  93  92  94  95  92  96  97  98  32  99 100   0   0   0   0   0\n    0   0   0   0   0   0   0]\n [101 102 103 104 105 106 107 108 109 110 111 112 113 114 106 115 116  61\n  117 118 119 120 110 121  61 122 123 117 124   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0]\n [ 49  77 125 106 126 127 128  59 129 130  97 131 132 133  11 134 135   2\n  136 137 138 139 140 141 142 143  11 134 144 101  75 145 103 146 147 148\n  149  14 150  49 151  46 152]\n [153 108 154  75 134 155 134 155 106 156 142  52  10 157 158 159 160 114\n  161  86  11 162 163 158 159 164 106 165 122 166 157  11 162 163   0   0\n    0   0   0   0   0   0   0]\n [167  15  14 168 169  77 170 171 125 107 172 148 173 174 148 175 176 177\n  178   9 179 180  11 181 182 125 107 183 184 185 186   0   0   0   0   0\n    0   0   0   0   0   0   0]]\n[[  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  11  16  17\n   18   2   1  19  20  21  22  23  24  25  26  27   0   0   0   0   0   0\n    0   0   0   0   0   0   0]\n [ 28  29  30  31  32  11  33  34  35  36  37  38  39  40  41  42  43  44\n   45  46  47  48  49  50  51  52  50  53  11  54  55  56  57   0   0   0\n    0   0   0   0   0   0   0]\n [ 58  59  60  61  62  63  64  65  66  67  68  69  70  71  62  71  72  73\n   74  75  76  77  78  79  80  81  82  83  62  84  85  86   0   0   0   0\n    0   0   0   0   0   0   0]\n [ 87  88  89  90  91  92  93  50  94  95  81  96  97  98  39  59  99  11\n  100 101 102 101 103 104 105 106 107 108 109 110 111   0   0   0   0   0\n    0   0   0   0   0   0   0]\n [112 113 114 115 116 117 118 119 120 121 122 123 124 125 117 126 127 128\n  129 130 131 132 121 133 128 134 135 129 136   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0]\n [ 50  83 137 117 138 139 140  62 141 142 143 144 145 146  11 147 148   2\n  149 150 151 152 153 154 155 156  11 147 157 158  81 159 114 160 161 162\n  163  14 164  50 165  47 166]\n [167 119 168  81 147 169 147 169 117 170 155  53 171 172 173 174 175 176\n  177 178  11 179 180 173 174 181 117 182 183 184 185  11 179 180   0   0\n    0   0   0   0   0   0   0]\n [186  15  14 187 188  83 189 190 137 118 191 162 192 193 162 194 195 196\n  197 198 199 200  74 201 202 137 118 203 204 205 206   0   0   0   0   0\n    0   0   0   0   0   0   0]]\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_batch(input_data, label_data, batch_size):\n",
    "    batch_num = len(input_data) // batch_size\n",
    "    for k in range(batch_num):\n",
    "        begin = k * batch_size\n",
    "        end = begin + batch_size\n",
    "        input_batch = input_data[begin: end]\n",
    "        label_batch = label_data[begin: end]\n",
    "        max_len = max([len(line) for line in input_batch])\n",
    "        input_batch = np.array([line + [0] * (max_len-len(line)) for line in input_batch])\n",
    "        label_batch = np.array([line + [0] * (max_len-len(line)) for line in label_batch])\n",
    "        yield input_batch, label_batch\n",
    "        \n",
    "batch = get_batch(input_num, label_num, 8)\n",
    "input_batch, label_batch = next(batch)\n",
    "\n",
    "print(input_batch)\n",
    "print(label_batch)"
   ],
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n"
   ],
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "\n",
    "# layer normalization\n",
    "\n",
    "def layer_normalization(inputs,\n",
    "              epsilon=1e-8,\n",
    "              scope='ln',\n",
    "              reuse=None):\n",
    "    '''\n",
    "    Applies layer normalization\n",
    "    \n",
    "    :param inputs: A tensor with 2 or more dimensions, \n",
    "                    where the first dimension has 'batch_size'\n",
    "    :param epsilon: A floating number. A very small \n",
    "                    number for preventing ZeroDivision Error\n",
    "    :param scope: Optional scope for 'variable_scope'\n",
    "    :param reuse: Boolean, whether to reuse the weights of \n",
    "                    a previous layer by the same name\n",
    "    :return: \n",
    "        A tensor with the same shape and data dtype as 'inputs'\n",
    "    '''\n",
    "    \n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        inputs_shape = inputs.get_shape()\n",
    "        params_shape = inputs_shape[-1:]\n",
    "        \n",
    "        mean, variance = tf.nn.moments(inputs, [-1], keep_dims=True)\n",
    "        beta = tf.Variable(tf.zeros(params_shape))\n",
    "        gamma = tf.Variable(tf.ones(params_shape))\n",
    "        normalized = (inputs - mean) / ((variance + epsilon)**0.5)\n",
    "        outputs = gamma * normalized + beta\n",
    "        \n",
    "    return outputs\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "\n",
    "# embedding\n",
    "\n",
    "def embedding(inputs,\n",
    "              vocab_size,\n",
    "              num_units,\n",
    "              zero_pad=True,\n",
    "              scale=True,\n",
    "              scope='embedding',\n",
    "              reuse=None):\n",
    "    \n",
    "    '''\n",
    "    Embeds a given tensor.\n",
    "    \n",
    "    :param inputs: A 'Tensor' with type 'int32' or 'int64' containing\n",
    "                    the ids to be looked up in 'lookup table'.\n",
    "    :param vocab_size: int. Vocabulary size\n",
    "    :param num_units: int. Number of embedding hidden units\n",
    "    :param zero_pad: boolean. If true, all the values of the first row\n",
    "                    (id 0) should be constant zeros.\n",
    "    :param scale: boolean, If true. the outputs is multiplied by sqrt\n",
    "                    num_units.\n",
    "    :param scope: Optional scope for 'variable_scope'\n",
    "    :param reuse: Boolean, whether to reuse the weights of a previous\n",
    "                layer by the same name.\n",
    "    :return: \n",
    "        A Tensor with one more rank than inputs`s. The last dimensionality\n",
    "        should be 'num_units'\n",
    "        \n",
    "        For example,\n",
    "\n",
    "    ```\n",
    "    import tensorflow as tf\n",
    "\n",
    "    inputs = tf.to_int32(tf.reshape(tf.range(2*3), (2, 3)))\n",
    "    outputs = embedding(inputs, 6, 2, zero_pad=True)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        print sess.run(outputs)\n",
    "    >>\n",
    "    [[[ 0.          0.        ]\n",
    "      [ 0.09754146  0.67385566]\n",
    "      [ 0.37864095 -0.35689294]]\n",
    "     [[-1.01329422 -1.09939694]\n",
    "      [ 0.7521342   0.38203377]\n",
    "      [-0.04973143 -0.06210355]]]\n",
    "    ```\n",
    "\n",
    "    ```\n",
    "    import tensorflow as tf\n",
    "\n",
    "    inputs = tf.to_int32(tf.reshape(tf.range(2*3), (2, 3)))\n",
    "    outputs = embedding(inputs, 6, 2, zero_pad=False)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        print sess.run(outputs)\n",
    "    >>\n",
    "    [[[-0.19172323 -0.39159766]\n",
    "      [-0.43212751 -0.66207761]\n",
    "      [ 1.03452027 -0.26704335]]\n",
    "     [[-0.11634696 -0.35983452]\n",
    "      [ 0.50208133  0.53509563]\n",
    "      [ 1.22204471 -0.96587461]]]\n",
    "    ```\n",
    "    '''\n",
    "    \n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        lookup_table = tf.get_variable('lookup_table',\n",
    "                                       dtype=tf.float32,\n",
    "                                       shape=[vocab_size, num_units],\n",
    "                                       initializer=tf.contrib.layers.xavier_initializer())\n",
    "        if zero_pad:\n",
    "            lookup_table = tf.concat((tf.zeros(shape=[1, num_units]),\n",
    "                                      lookup_table[1:, :]), 0)\n",
    "        outputs = tf.nn.embedding_lookup(lookup_table, inputs)\n",
    "        \n",
    "        if scale:\n",
    "            outputs = outputs * (num_units ** 0.5)\n",
    "            \n",
    "        \n",
    "    return outputs\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "\n",
    "# multi-head attention\n",
    "\n",
    "def multihead_attention(emb,\n",
    "                        queries,\n",
    "                        keys,\n",
    "                        num_units=None,\n",
    "                        num_heads=8,\n",
    "                        dropout_rate=0,\n",
    "                        is_training=True,\n",
    "                        causality=False,\n",
    "                        scope='multihead_attention',\n",
    "                        reuse=None):\n",
    "    \n",
    "    '''\n",
    "    Applies multihead attention.\n",
    "    \n",
    "    :param emb: \n",
    "    :param queries: A 3d tensor with shape of [N, T_q, C_q].\n",
    "    :param keys: A 3d tensor with shape of [N, T_k, C_k].\n",
    "    :param num_units: A scalar. Attention size.\n",
    "    :param num_heads: int. Number of heads.\n",
    "    :param dropout_rate: A floating point number.\n",
    "    :param is_training: Boolean. Controller of mechanism for dropout.\n",
    "    :param causality: boolean. If true, units that reference the future\n",
    "                        are masked.\n",
    "    :param scope: Optional scope for 'variable_scope'\n",
    "    :param reuse: boolean, whether to reuse the weights of a previous layer\n",
    "                    by the same name.\n",
    "    :return: \n",
    "            A 3d tensor with shape of (N, T_q, C)\n",
    "    '''\n",
    "    \n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        # Set the fall back option for num_units\n",
    "        if num_units is None:\n",
    "            num_units = queries.get_shape().as_list[-1]\n",
    "            \n",
    "        # Linear projections\n",
    "        Q = tf.layers.dense(queries, num_units, activation=tf.nn.relu) # (N, T_q, C)\n",
    "        K = tf.layers.dense(keys, num_units, activation=tf.nn.relu) # (N, T_k, C)\n",
    "        V = tf.layers.dense(keys, num_units, activation=tf.nn.relu) # (N, T_k, C)\n",
    "        \n",
    "        # Split and concat\n",
    "        Q_concat = tf.concat(tf.split(Q, num_heads, axis=2), axis=0) # (h*N, T_q, C/h)\n",
    "        K_concat = tf.concat(tf.split(K, num_heads, axis=2), axis=0) # (h*N, T_k, C/h)\n",
    "        V_concat = tf.concat(tf.split(V, num_heads, axis=2), axis=0) # (h*N, T_k, C/h)\n",
    "        \n",
    "        # Multiplication\n",
    "        attention = tf.matmul(Q_concat, tf.transpose(K_concat, [0, 2, 1])) # (h*N, T_q, T_k)\n",
    "        \n",
    "        # Scale\n",
    "        attention = attention / (K_concat.get_shape().as_list()[-1] ** 0.5)\n",
    "        \n",
    "        # Key Masking\n",
    "        key_masks = tf.sign(tf.abs(tf.reduce_sum(emb, axis=-1))) # (N, T_k)\n",
    "        key_masks = tf.tile(key_masks, [num_heads, 1]) # (h*N, T_k)\n",
    "        key_masks = tf.tile(tf.expand_dims(key_masks, 1), [1, tf.shape(queries)[1], 1]) # (h*N, T_q, T_K)\n",
    "        \n",
    "        paddings = tf.ones_like(attention) * (-2**32+1)\n",
    "        attention = tf.where(tf.equal(key_masks, 0), paddings, attention) # (h*N, T_q, T_k)\n",
    "        \n",
    "        # Causality = Future blinding\n",
    "        if causality:\n",
    "            diag_vals = tf.ones_like(attention[0, :, :]) # (T_q, T_k)\n",
    "            tril = tf.linalg.LinearOperatorLowerTriangular(diag_vals).to_dense() # (T_q, T_k)\n",
    "            masks = tf.tile(tf.expand_dims(tril, 0), [tf.shape(attention)[0], 1, 1]) # (h*N, T_q, T_k)\n",
    "            \n",
    "            paddings = tf.ones_like(masks) * (-2 ** 32 + 1)\n",
    "            attention = tf.where(tf.equal(masks, 0), paddings, attention) # (h*N, T_q, T_k)\n",
    "            \n",
    "        # Activation\n",
    "        attention = tf.nn.softmax(attention) # (h*N, T_q, T_k)\n",
    "        \n",
    "        # Query Masking\n",
    "        query_masks = tf.sign(tf.abs(tf.reduce_sum(emb, axis=-1))) # (N, T_q)\n",
    "        query_masks = tf.tile(query_masks, [num_heads, 1]) # (h*N, T_q)\n",
    "        query_masks = tf.tile(tf.expand_dims(query_masks, -1), [1, 1, tf.shape(keys)[1]]) # (h*N, T_q, T_k)\n",
    "        attention *= query_masks # broadcasting. (N, T_q, C)\n",
    "        \n",
    "        # Dropouts\n",
    "        attention = tf.layers.dropout(attention, rate=dropout_rate, training=tf.convert_to_tensor(is_training))\n",
    "        \n",
    "        # Weighted sum\n",
    "        outputs = tf.matmul(attention, V_concat) # (h*N, T_q, C/h)\n",
    "        \n",
    "        # Restore shape\n",
    "        outputs = tf.concat(tf.split(outputs, num_heads, axis=0), axis=2) # (N, T_q, C)\n",
    "        \n",
    "        # Residual connection\n",
    "        outputs += queries\n",
    "        \n",
    "        # Normalization\n",
    "        outputs = layer_normalization(outputs) # (N, T_q, C)\n",
    "        \n",
    "    return outputs\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "\n",
    "def feedforward(inputs,\n",
    "                num_units=[2048, 512],\n",
    "                scope='multihead_attention',\n",
    "                reuse=None):\n",
    "    \n",
    "    '''\n",
    "    Point-wise feed forward net.\n",
    "    \n",
    "    :param inputs: A 3d tensor with shape of [N, T, C].\n",
    "    :param num_units: A list of two integers.\n",
    "    :param scope: Optional scope for 'variable_scope'\n",
    "    :param reuse: Boolean, whether to reuse the weights \n",
    "                of a previous layer by the same name.\n",
    "    :return: \n",
    "    A 3d tensor with the same shape and dtype as inputs\n",
    "    '''\n",
    "    \n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        # Inner layer\n",
    "        params = {'inputs': inputs, 'filters': num_units[0],\n",
    "                  'kernel_size': 1, 'activation': tf.nn.relu,\n",
    "                  'use_bias': True}\n",
    "        \n",
    "        outputs = tf.layers.conv1d(**params)\n",
    "        \n",
    "        # Readout layer\n",
    "        params = {'inputs': outputs, 'filters': num_units[1],\n",
    "                  'kernel_size': 1, 'activation': None,\n",
    "                  'use_bias': True}\n",
    "        \n",
    "        outputs = tf.layers.conv1d(**params)\n",
    "        \n",
    "        # Residual connection\n",
    "        outputs += inputs\n",
    "        \n",
    "        # Normalize\n",
    "        outputs = layer_normalization(outputs)\n",
    "        \n",
    "    return outputs\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "\n",
    "def label_smoothing(inputs, epsilon=0.1):\n",
    "    '''\n",
    "    Applies label smoothing.\n",
    "    \n",
    "    :param inputs: A 3d tensor with shape of [N, T, V],\n",
    "                where V is the number of vocabulary\n",
    "    :param epsilon: Smoothing rate.\n",
    "    :return: \n",
    "    \n",
    "    For example,\n",
    "    \n",
    "    ```\n",
    "    import tensorflow as tf\n",
    "    inputs = tf.convert_to_tensor([[[0, 0, 1], \n",
    "       [0, 1, 0],\n",
    "       [1, 0, 0]],\n",
    "      [[1, 0, 0],\n",
    "       [1, 0, 0],\n",
    "       [0, 1, 0]]], tf.float32)\n",
    "       \n",
    "    outputs = label_smoothing(inputs)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        print(sess.run([outputs]))\n",
    "    \n",
    "    >>\n",
    "    [array([[[ 0.03333334,  0.03333334,  0.93333334],\n",
    "        [ 0.03333334,  0.93333334,  0.03333334],\n",
    "        [ 0.93333334,  0.03333334,  0.03333334]],\n",
    "       [[ 0.93333334,  0.03333334,  0.03333334],\n",
    "        [ 0.93333334,  0.03333334,  0.03333334],\n",
    "        [ 0.03333334,  0.93333334,  0.03333334]]], dtype=float32)]   \n",
    "    ```\n",
    "    '''\n",
    "    \n",
    "    K = inputs.get_shape().as_list()[-1] # number of channels\n",
    "    \n",
    "    return ((1-epsilon) * inputs) + (epsilon / K)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "\n",
    "class Graph():\n",
    "    def __init__(self, arg, is_training=True):\n",
    "        tf.reset_default_graph()\n",
    "        self.is_training = arg.is_training\n",
    "        self.hidden_units = arg.hidden_units\n",
    "        self.input_vocab_size = arg.input_vocab_size\n",
    "        self.label_vocab_size = arg.label_vocab_size\n",
    "        self.num_heads = arg.num_heads\n",
    "        self.num_blocks = arg.num_blocks\n",
    "        self.max_length = arg.max_length\n",
    "        self.lr = arg.lr\n",
    "        self.dropout_rate = arg.dropout_rate\n",
    "        \n",
    "        # input\n",
    "        self.x = tf.placeholder(tf.int32, shape=(None, None))\n",
    "        self.y = tf.placeholder(tf.int32, shape=(None, None))\n",
    "        \n",
    "        # embedding\n",
    "        self.emb = embedding(self.x, \n",
    "                             vocab_size=self.input_vocab_size,\n",
    "                             num_units=self.hidden_units,\n",
    "                             scale=True, scope='enc_embed')\n",
    "        \n",
    "        self.enc = self.emb + embedding(tf.tile(tf.expand_dims(tf.range(tf.shape(self.x)[1]), 0), [tf.shape(self.x)[0], 1]),\n",
    "                                        vocab_size=self.max_length, num_units=self.hidden_units,\n",
    "                                        zero_pad=False, scale=False, scope='enc_pe')\n",
    "        \n",
    "        # Dropout\n",
    "        self.enc = tf.layers.dropout(self.enc,\n",
    "                                     rate=self.dropout_rate,\n",
    "                                     training=tf.convert_to_tensor(self.is_training))\n",
    "        \n",
    "        # Blocks\n",
    "        for i in range(self.num_blocks):\n",
    "            with tf.variable_scope(f'num_blocks_{i}'):\n",
    "                # Multihead Attention\n",
    "                self.enc = multihead_attention(emb=self.emb,\n",
    "                                               queries=self.enc,\n",
    "                                               keys=self.enc,\n",
    "                                               num_units=self.hidden_units,\n",
    "                                               num_heads=self.num_heads,\n",
    "                                               dropout_rate=self.dropout_rate,\n",
    "                                               is_training=self.is_training,\n",
    "                                               causality=False)\n",
    "                \n",
    "        # Feed Forward\n",
    "        self.outputs = feedforward(self.enc, num_units=[4*self.hidden_units, self.hidden_units])\n",
    "        \n",
    "        # Final linear projection\n",
    "        self.logits = tf.layers.dense(self.outputs, self.label_vocab_size)\n",
    "        self.preds = tf.to_int32(tf.argmax(self.logits, axis=-1))\n",
    "        # 去掉填充的部分\n",
    "        self.istarget = tf.to_float(tf.not_equal(self.y, 0))\n",
    "        self.acc = tf.reduce_sum(tf.to_float(tf.equal(self.preds, self.y)) * self.istarget) / (tf.reduce_sum(self.istarget))\n",
    "        tf.summary.scalar('acc', self.acc)\n",
    "        \n",
    "        if is_training:\n",
    "            # Loss\n",
    "            self.y_smoothed = label_smoothing(tf.one_hot(self.y, depth=self.label_vocab_size))\n",
    "            self.loss = tf.nn.softmax_cross_entropy_with_logits(logits=self.logits, labels=self.y_smoothed)\n",
    "            self.mean_loss = tf.reduce_sum(self.loss * self.istarget) / (tf.reduce_sum(self.istarget))\n",
    "            \n",
    "            # Training Scheme\n",
    "            self.global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "            self.optimizer = tf.train.AdamOptimizer(learning_rate=self.lr, beta1=0.9, beta2=0.98, epsilon=1e-8)\n",
    "            self.train_op = self.optimizer.minimize(self.mean_loss, global_step=self.global_step)\n",
    "            \n",
    "            tf.summary.scalar('mean_loss', self.mean_loss)\n",
    "            self.merged = tf.summary.merge_all()\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "\n",
    "def create_hparams():\n",
    "    params = tf.contrib.training.HParams(num_heads=8,\n",
    "                                         num_blocks=6,\n",
    "                                         input_vocab_size=50,\n",
    "                                         label_vocab_size=50,\n",
    "                                         max_length=100,\n",
    "                                         hidden_units=512,\n",
    "                                         dropout_rate=0.2,\n",
    "                                         lr=0.0003,\n",
    "                                         is_training=True)\n",
    "    \n",
    "    return params\n",
    "\n",
    "arg = create_hparams()\n",
    "arg.input_vocab_size = len(pny2id)\n",
    "arg.label_vocab_size = len(word2id)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:From <ipython-input-12-a00ac296e0aa>:60: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\n\nFuture major versions of TensorFlow will allow gradients to flow\ninto the labels input on backprop by default.\n\nSee `tf.nn.softmax_cross_entropy_with_logits_v2`.\n\n",
      "epochs 1 : average loss = 1.7576312879450506\n",
      "epochs 2 : average loss = 1.5597799746569696\n",
      "epochs 3 : average loss = 1.509507176207393\n",
      "epochs 4 : average loss = 1.4711973466739237\n",
      "epochs 5 : average loss = 1.443885849163287\n",
      "epochs 6 : average loss = 1.424225844267428\n",
      "epochs 7 : average loss = 1.407690567253346\n",
      "epochs 8 : average loss = 1.3937231061971664\n",
      "epochs 9 : average loss = 1.3822234949756387\n",
      "epochs 10 : average loss = 1.372285391564509\n",
      "epochs 11 : average loss = 1.3636000377981192\n",
      "epochs 12 : average loss = 1.3566907827935895\n",
      "epochs 13 : average loss = 1.3501691292357416\n",
      "epochs 14 : average loss = 1.3442635642059182\n",
      "epochs 15 : average loss = 1.3388745027945486\n",
      "epochs 16 : average loss = 1.3339690614071624\n",
      "epochs 17 : average loss = 1.3294396614644313\n",
      "epochs 18 : average loss = 1.3254647251989466\n",
      "epochs 19 : average loss = 1.3214772410847735\n",
      "epochs 20 : average loss = 1.31827114781581\n",
      "epochs 21 : average loss = 1.3143856748476939\n",
      "epochs 22 : average loss = 1.311377807276055\n",
      "epochs 23 : average loss = 1.308357387818073\n",
      "epochs 24 : average loss = 1.305533264797121\n",
      "epochs 25 : average loss = 1.3029258111349467\n",
      "epochs 26 : average loss = 1.299980278696614\n",
      "epochs 27 : average loss = 1.2977846606316423\n",
      "epochs 28 : average loss = 1.295163657339286\n",
      "epochs 29 : average loss = 1.293484698440949\n",
      "epochs 30 : average loss = 1.2910786823810796\n",
      "epochs 31 : average loss = 1.2892857366039554\n",
      "epochs 32 : average loss = 1.2871067929691344\n",
      "epochs 33 : average loss = 1.2852096063743097\n",
      "epochs 34 : average loss = 1.2835374994742776\n",
      "epochs 35 : average loss = 1.2815174801888323\n",
      "epochs 36 : average loss = 1.2802571836260417\n",
      "epochs 37 : average loss = 1.2785401287181275\n",
      "epochs 38 : average loss = 1.2774938374499258\n",
      "epochs 39 : average loss = 1.275845970806136\n",
      "epochs 40 : average loss = 1.2742418499339567\n",
      "epochs 41 : average loss = 1.2729235500011105\n",
      "epochs 42 : average loss = 1.272041666660364\n",
      "epochs 43 : average loss = 1.270658009065079\n",
      "epochs 44 : average loss = 1.2695541938736832\n",
      "epochs 45 : average loss = 1.2680732524380216\n",
      "epochs 46 : average loss = 1.2670690494349848\n",
      "epochs 47 : average loss = 1.2659243837657521\n",
      "epochs 48 : average loss = 1.26487338897464\n",
      "epochs 49 : average loss = 1.263632839201699\n",
      "epochs 50 : average loss = 1.2627062902426927\n",
      "训练时间为: 72.45\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "epochs = 50\n",
    "batch_size = 64\n",
    "\n",
    "g = Graph(arg)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    merged = tf.summary.merge_all()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    if os.path.exists('../logs/model.meta'):\n",
    "        saver.restore(sess, '../logs/model')\n",
    "    \n",
    "    writer = tf.summary.FileWriter('../tensorboard/lm', tf.get_default_graph())\n",
    "    for k in range(epochs):\n",
    "        total_loss = 0\n",
    "        batch_num = len(input_num) // batch_size\n",
    "        batch = get_batch(input_num, label_num, batch_size)\n",
    "        for i in range(batch_num):\n",
    "            input_batch, label_batch = next(batch)\n",
    "            feed = {g.x: input_batch, g.y: label_batch}\n",
    "            cost, _ = sess.run([g.mean_loss, g.train_op], feed_dict=feed)\n",
    "            total_loss += cost\n",
    "            if (k * batch_num + i) % 10 == 0:\n",
    "                rs = sess.run(merged, feed_dict=feed)\n",
    "                writer.add_summary(rs, k*batch_num + i)\n",
    "                \n",
    "        print('epochs', k+1, ': average loss =', total_loss/batch_num)\n",
    "            \n",
    "    saver.save(sess, '../logs/model')\n",
    "    writer.close()\n",
    "\n",
    "print(f'训练时间为: {(time.time()-start)/60:.2f}')\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../logs/model\n",
      "['zhou1', 'jie2', 'lun2']\n周杰伦\n",
      "['lin2', 'jun4', 'jie2']\n林俊杰\n",
      "['wang2', 'jun4', 'kai3']\n王俊凯\n",
      "['si4', 'xiao3', 'hua1', 'dan4']\n四小花旦\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "\n",
    "arg.is_training = False\n",
    "\n",
    "g = Graph(arg)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, '../logs/model')\n",
    "    while True:\n",
    "        line = input('输入测试拼音: ')\n",
    "        if line == 'exit': break\n",
    "        line = line.strip('\\n').split(' ')\n",
    "        x = np.array([pny2id.index(pny) for pny in line])\n",
    "        x = x.reshape(1, -1)\n",
    "        preds = sess.run(g.preds, {g.x: x})\n",
    "        result = ''.join(word2id[idx] for idx in preds[0])\n",
    "        print(line)\n",
    "        print(result)\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}