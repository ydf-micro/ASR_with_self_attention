{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "encoder_embedding_input:\ntf.Tensor(\n[[[0.12       0.22       0.32       0.42      ]\n  [0.22999999 0.33       0.43       0.53      ]\n  [0.34       0.44       0.53999996 0.64      ]\n  [0.45       0.55       0.65000004 0.75      ]]\n\n [[0.32       0.42       0.52       0.62      ]\n  [0.43       0.53       0.63       0.72999996]\n  [0.14       0.24       0.34       0.44      ]\n  [0.25       0.35       0.45       0.55      ]]], shape=(2, 4, 4), dtype=float32)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "\n",
    "chinese_embedding = tf.constant([[0.11, 0.21,0.31,0.41],\n",
    "                                 [0.21,0.31,0.41,0.51],\n",
    "                                 [0.31,0.41,0.51,0.61],\n",
    "                                 [0.41,0.51,0.61,0.71]], dtype=tf.float32)\n",
    "\n",
    "position_embedding = tf.constant([[0.01,0.01,0.01,0.01],\n",
    "                                  [0.02,0.02,0.02,0.02],\n",
    "                                  [0.03,0.03,0.03,0.03],\n",
    "                                  [0.04,0.04,0.04,0.04]], dtype=tf.float32)\n",
    "\n",
    "encoder_input = tf.constant([[0,1,2,3],[2,3,0,1]], dtype=tf.int32)\n",
    "\n",
    "with tf.variable_scope('encoder_input'):\n",
    "    encoder_embedding_input = tf.nn.embedding_lookup(chinese_embedding, encoder_input)\n",
    "    encoder_embedding_input += position_embedding\n",
    "    \n",
    "print('encoder_embedding_input:')\n",
    "print(encoder_embedding_input)\n"
   ],
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "encoder_Q:\ntf.Tensor(\n[[0.32       0.428      0.536      0.644      0.752      0.85999995]\n [0.43       0.582      0.734      0.886      1.038      1.1899999 ]\n [0.54       0.73599994 0.932      1.128      1.324      1.52      ]\n [0.65000004 0.89       1.1300001  1.37       1.61       1.85      ]\n [0.52       0.708      0.896      1.084      1.272      1.46      ]\n [0.63       0.862      1.094      1.326      1.558      1.79      ]\n [0.34       0.456      0.572      0.68799996 0.804      0.91999996]\n [0.45000002 0.61       0.77000004 0.93       1.09       1.25      ]], shape=(8, 6), dtype=float32)\nencoder_K:\ntf.Tensor(\n[[0.29839998 0.4064     0.5144     0.6224     0.73039997 0.83839995]\n [0.3996     0.5516     0.7036     0.8556     1.0076     1.1596    ]\n [0.5008     0.6968     0.8927999  1.0888     1.2847999  1.4807999 ]\n [0.602      0.842      1.082      1.322      1.562      1.8019999 ]\n [0.4824     0.67039996 0.8584     1.0464     1.2343999  1.4224    ]\n [0.5836     0.8156     1.0475999  1.2795999  1.5115999  1.7435999 ]\n [0.3168     0.4328     0.5488     0.6648     0.7808     0.8968    ]\n [0.418      0.578      0.738      0.898      1.058      1.2179999 ]], shape=(8, 6), dtype=float32)\nencoder_V:\ntf.Tensor(\n[[0.34159997 0.44959998 0.55759996 0.6656     0.7736     0.8816    ]\n [0.4604     0.61239994 0.76439995 0.9164     1.0684     1.2204    ]\n [0.57919997 0.77519995 0.9711999  1.1672     1.3632     1.5591999 ]\n [0.698      0.938      1.178      1.418      1.658      1.8980001 ]\n [0.55759996 0.7456     0.93359995 1.1216     1.3096     1.4976001 ]\n [0.67639995 0.90839994 1.1403999  1.3723999  1.6043999  1.8364    ]\n [0.36319998 0.4792     0.5952     0.7112     0.8272     0.9432    ]\n [0.482      0.64199996 0.802      0.962      1.122      1.282     ]], shape=(8, 6), dtype=float32)\n\nafter reshape:\nencoder_Q:\ntf.Tensor(\n[[[0.32       0.428      0.536      0.644      0.752      0.85999995]\n  [0.43       0.582      0.734      0.886      1.038      1.1899999 ]\n  [0.54       0.73599994 0.932      1.128      1.324      1.52      ]\n  [0.65000004 0.89       1.1300001  1.37       1.61       1.85      ]]\n\n [[0.52       0.708      0.896      1.084      1.272      1.46      ]\n  [0.63       0.862      1.094      1.326      1.558      1.79      ]\n  [0.34       0.456      0.572      0.68799996 0.804      0.91999996]\n  [0.45000002 0.61       0.77000004 0.93       1.09       1.25      ]]], shape=(2, 4, 6), dtype=float32)\nencoder_K:\ntf.Tensor(\n[[[0.29839998 0.4064     0.5144     0.6224     0.73039997 0.83839995]\n  [0.3996     0.5516     0.7036     0.8556     1.0076     1.1596    ]\n  [0.5008     0.6968     0.8927999  1.0888     1.2847999  1.4807999 ]\n  [0.602      0.842      1.082      1.322      1.562      1.8019999 ]]\n\n [[0.4824     0.67039996 0.8584     1.0464     1.2343999  1.4224    ]\n  [0.5836     0.8156     1.0475999  1.2795999  1.5115999  1.7435999 ]\n  [0.3168     0.4328     0.5488     0.6648     0.7808     0.8968    ]\n  [0.418      0.578      0.738      0.898      1.058      1.2179999 ]]], shape=(2, 4, 6), dtype=float32)\nencoder_V:\ntf.Tensor(\n[[[0.34159997 0.44959998 0.55759996 0.6656     0.7736     0.8816    ]\n  [0.4604     0.61239994 0.76439995 0.9164     1.0684     1.2204    ]\n  [0.57919997 0.77519995 0.9711999  1.1672     1.3632     1.5591999 ]\n  [0.698      0.938      1.178      1.418      1.658      1.8980001 ]]\n\n [[0.55759996 0.7456     0.93359995 1.1216     1.3096     1.4976001 ]\n  [0.67639995 0.90839994 1.1403999  1.3723999  1.6043999  1.8364    ]\n  [0.36319998 0.4792     0.5952     0.7112     0.8272     0.9432    ]\n  [0.482      0.64199996 0.802      0.962      1.122      1.282     ]]], shape=(2, 4, 6), dtype=float32)\n",
      "\nattention_map:\ntf.Tensor(\n[[[0.21250255 0.23575781 0.261558   0.29018164]\n  [0.19920368 0.22981207 0.26512352 0.30586073]\n  [0.18638706 0.22359636 0.26823393 0.32178268]\n  [0.17407157 0.21714526 0.27087748 0.33790562]]\n\n [[0.26062688 0.31044313 0.19575655 0.2331735 ]\n  [0.26206347 0.32459578 0.18464126 0.22869946]\n  [0.2575238  0.2877434  0.21476535 0.2399674 ]\n  [0.25953028 0.30154324 0.20302995 0.23589654]]], shape=(2, 4, 4), dtype=float32)\n\nencoder_first_as_output:\ntf.Tensor(\n[[[0.5351749  0.7148693  0.89456373 1.0742582  1.2539526  1.433647  ]\n  [0.54090375 0.72271997 0.9045362  1.0863523  1.2681686  1.4499849 ]\n  [0.546579   0.7304971  0.9144152  1.0983334  1.2822515  1.4661697 ]\n  [0.5521869  0.738182   0.9241771  1.1101723  1.2961675  1.4821626 ]]\n\n [[0.5387977  0.71983385 0.9008701  1.0819063  1.2629423  1.4439785 ]\n  [0.542978   0.7255624  0.90814686 1.0907314  1.2733158  1.4559004 ]\n  [0.53189194 0.7103704  0.88884896 1.0673275  1.245806   1.4242845 ]\n  [0.53612053 0.7161651  0.8962098  1.0762545  1.2562991  1.4363437 ]]], shape=(2, 4, 6), dtype=float32)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "encoder_w_Q = tf.constant([[0.1, 0.2, 0.3, 0.4, 0.5, 0.6],\n",
    "                           [0.2, 0.3, 0.4, 0.5, 0.6, 0.7],\n",
    "                           [0.3, 0.4, 0.5, 0.6, 0.7, 0.8],\n",
    "                           [0.4, 0.5, 0.6, 0.7, 0.8, 0.9]], dtype=tf.float32)\n",
    "\n",
    "encoder_w_K = tf.constant([[0.08, 0.18, 0.28, 0.38, 0.48, 0.58],\n",
    "                           [0.18, 0.28, 0.38, 0.48, 0.58, 0.68],\n",
    "                           [0.28, 0.38, 0.48, 0.58, 0.68, 0.78],\n",
    "                           [0.38, 0.48, 0.58, 0.68, 0.78, 0.88]], dtype=tf.float32)\n",
    "\n",
    "encoder_w_V = tf.constant([[0.12, 0.22, 0.32, 0.42, 0.52, 0.62],\n",
    "                           [0.22, 0.32, 0.42, 0.52, 0.62, 0.72],\n",
    "                           [0.32, 0.42, 0.52, 0.62, 0.72, 0.82],\n",
    "                           [0.42, 0.52, 0.62, 0.72, 0.82, 0.92]], dtype=tf.float32)\n",
    "\n",
    "with tf.variable_scope('encoder_scaled_dot_product_attention'):\n",
    "    encoder_Q = tf.matmul(tf.reshape(encoder_embedding_input, (-1, tf.shape(encoder_embedding_input)[2])), encoder_w_Q)\n",
    "    encoder_K = tf.matmul(tf.reshape(encoder_embedding_input, (-1, tf.shape(encoder_embedding_input)[2])), encoder_w_K)\n",
    "    encoder_V = tf.matmul(tf.reshape(encoder_embedding_input, (-1, tf.shape(encoder_embedding_input)[2])), encoder_w_V)\n",
    "    \n",
    "    print('encoder_Q:')\n",
    "    print(encoder_Q)\n",
    "    print('encoder_K:')\n",
    "    print(encoder_K)\n",
    "    print('encoder_V:')\n",
    "    print(encoder_V)\n",
    "    \n",
    "    encoder_Q = tf.reshape(encoder_Q, (tf.shape(encoder_embedding_input)[0], tf.shape(encoder_embedding_input)[1], -1))\n",
    "    encoder_K = tf.reshape(encoder_K, (tf.shape(encoder_embedding_input)[0], tf.shape(encoder_embedding_input)[1], -1))\n",
    "    encoder_V = tf.reshape(encoder_V, (tf.shape(encoder_embedding_input)[0], tf.shape(encoder_embedding_input)[1], -1))\n",
    "    \n",
    "    print()\n",
    "    print('after reshape:')\n",
    "    print('encoder_Q:')\n",
    "    print(encoder_Q)\n",
    "    print('encoder_K:')\n",
    "    print(encoder_K)\n",
    "    print('encoder_V:')\n",
    "    print(encoder_V)\n",
    "    \n",
    "    attention_map = tf.matmul(encoder_Q, tf.transpose(encoder_K, [0, 2, 1]))\n",
    "    attention_map /= 8\n",
    "    attention_map = tf.nn.softmax(attention_map)\n",
    "    \n",
    "    print()\n",
    "    print('attention_map:')\n",
    "    print(attention_map)\n",
    "    \n",
    "    encoder_first_as_output = tf.matmul(attention_map, encoder_V)\n",
    "    \n",
    "    print()\n",
    "    print('encoder_first_as_output:')\n",
    "    print(encoder_first_as_output)\n",
    "    "
   ],
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Multi-Head Attention:\nencoder_Q:\ntf.Tensor(\n[[0.32       0.428      0.536      0.644      0.752      0.85999995]\n [0.43       0.582      0.734      0.886      1.038      1.1899999 ]\n [0.54       0.73599994 0.932      1.128      1.324      1.52      ]\n [0.65000004 0.89       1.1300001  1.37       1.61       1.85      ]\n [0.52       0.708      0.896      1.084      1.272      1.46      ]\n [0.63       0.862      1.094      1.326      1.558      1.79      ]\n [0.34       0.456      0.572      0.68799996 0.804      0.91999996]\n [0.45000002 0.61       0.77000004 0.93       1.09       1.25      ]], shape=(8, 6), dtype=float32)\nencoder_K:\ntf.Tensor(\n[[0.29839998 0.4064     0.5144     0.6224     0.73039997 0.83839995]\n [0.3996     0.5516     0.7036     0.8556     1.0076     1.1596    ]\n [0.5008     0.6968     0.8927999  1.0888     1.2847999  1.4807999 ]\n [0.602      0.842      1.082      1.322      1.562      1.8019999 ]\n [0.4824     0.67039996 0.8584     1.0464     1.2343999  1.4224    ]\n [0.5836     0.8156     1.0475999  1.2795999  1.5115999  1.7435999 ]\n [0.3168     0.4328     0.5488     0.6648     0.7808     0.8968    ]\n [0.418      0.578      0.738      0.898      1.058      1.2179999 ]], shape=(8, 6), dtype=float32)\nencoder_V:\ntf.Tensor(\n[[0.34159997 0.44959998 0.55759996 0.6656     0.7736     0.8816    ]\n [0.4604     0.61239994 0.76439995 0.9164     1.0684     1.2204    ]\n [0.57919997 0.77519995 0.9711999  1.1672     1.3632     1.5591999 ]\n [0.698      0.938      1.178      1.418      1.658      1.8980001 ]\n [0.55759996 0.7456     0.93359995 1.1216     1.3096     1.4976001 ]\n [0.67639995 0.90839994 1.1403999  1.3723999  1.6043999  1.8364    ]\n [0.36319998 0.4792     0.5952     0.7112     0.8272     0.9432    ]\n [0.482      0.64199996 0.802      0.962      1.122      1.282     ]], shape=(8, 6), dtype=float32)\n\nafter reshape:\nencoder_Q:\ntf.Tensor(\n[[[0.32       0.428      0.536      0.644      0.752      0.85999995]\n  [0.43       0.582      0.734      0.886      1.038      1.1899999 ]\n  [0.54       0.73599994 0.932      1.128      1.324      1.52      ]\n  [0.65000004 0.89       1.1300001  1.37       1.61       1.85      ]]\n\n [[0.52       0.708      0.896      1.084      1.272      1.46      ]\n  [0.63       0.862      1.094      1.326      1.558      1.79      ]\n  [0.34       0.456      0.572      0.68799996 0.804      0.91999996]\n  [0.45000002 0.61       0.77000004 0.93       1.09       1.25      ]]], shape=(2, 4, 6), dtype=float32)\nencoder_K:\ntf.Tensor(\n[[[0.29839998 0.4064     0.5144     0.6224     0.73039997 0.83839995]\n  [0.3996     0.5516     0.7036     0.8556     1.0076     1.1596    ]\n  [0.5008     0.6968     0.8927999  1.0888     1.2847999  1.4807999 ]\n  [0.602      0.842      1.082      1.322      1.562      1.8019999 ]]\n\n [[0.4824     0.67039996 0.8584     1.0464     1.2343999  1.4224    ]\n  [0.5836     0.8156     1.0475999  1.2795999  1.5115999  1.7435999 ]\n  [0.3168     0.4328     0.5488     0.6648     0.7808     0.8968    ]\n  [0.418      0.578      0.738      0.898      1.058      1.2179999 ]]], shape=(2, 4, 6), dtype=float32)\nencoder_V:\ntf.Tensor(\n[[[0.34159997 0.44959998 0.55759996 0.6656     0.7736     0.8816    ]\n  [0.4604     0.61239994 0.76439995 0.9164     1.0684     1.2204    ]\n  [0.57919997 0.77519995 0.9711999  1.1672     1.3632     1.5591999 ]\n  [0.698      0.938      1.178      1.418      1.658      1.8980001 ]]\n\n [[0.55759996 0.7456     0.93359995 1.1216     1.3096     1.4976001 ]\n  [0.67639995 0.90839994 1.1403999  1.3723999  1.6043999  1.8364    ]\n  [0.36319998 0.4792     0.5952     0.7112     0.8272     0.9432    ]\n  [0.482      0.64199996 0.802      0.962      1.122      1.282     ]]], shape=(2, 4, 6), dtype=float32)\nencoder_Q_split:\n[<tf.Tensor: id=2183, shape=(2, 4, 3), dtype=float32, numpy=\narray([[[0.32      , 0.428     , 0.536     ],\n        [0.43      , 0.582     , 0.734     ],\n        [0.54      , 0.73599994, 0.932     ],\n        [0.65000004, 0.89      , 1.1300001 ]],\n\n       [[0.52      , 0.708     , 0.896     ],\n        [0.63      , 0.862     , 1.094     ],\n        [0.34      , 0.456     , 0.572     ],\n        [0.45000002, 0.61      , 0.77000004]]], dtype=float32)>, <tf.Tensor: id=2184, shape=(2, 4, 3), dtype=float32, numpy=\narray([[[0.644     , 0.752     , 0.85999995],\n        [0.886     , 1.038     , 1.1899999 ],\n        [1.128     , 1.324     , 1.52      ],\n        [1.37      , 1.61      , 1.85      ]],\n\n       [[1.084     , 1.272     , 1.46      ],\n        [1.326     , 1.558     , 1.79      ],\n        [0.68799996, 0.804     , 0.91999996],\n        [0.93      , 1.09      , 1.25      ]]], dtype=float32)>]\nencoder_K_split:\n[<tf.Tensor: id=2187, shape=(2, 4, 3), dtype=float32, numpy=\narray([[[0.29839998, 0.4064    , 0.5144    ],\n        [0.3996    , 0.5516    , 0.7036    ],\n        [0.5008    , 0.6968    , 0.8927999 ],\n        [0.602     , 0.842     , 1.082     ]],\n\n       [[0.4824    , 0.67039996, 0.8584    ],\n        [0.5836    , 0.8156    , 1.0475999 ],\n        [0.3168    , 0.4328    , 0.5488    ],\n        [0.418     , 0.578     , 0.738     ]]], dtype=float32)>, <tf.Tensor: id=2188, shape=(2, 4, 3), dtype=float32, numpy=\narray([[[0.6224    , 0.73039997, 0.83839995],\n        [0.8556    , 1.0076    , 1.1596    ],\n        [1.0888    , 1.2847999 , 1.4807999 ],\n        [1.322     , 1.562     , 1.8019999 ]],\n\n       [[1.0464    , 1.2343999 , 1.4224    ],\n        [1.2795999 , 1.5115999 , 1.7435999 ],\n        [0.6648    , 0.7808    , 0.8968    ],\n        [0.898     , 1.058     , 1.2179999 ]]], dtype=float32)>]\nencoder_V_split:\n[<tf.Tensor: id=2191, shape=(2, 4, 3), dtype=float32, numpy=\narray([[[0.34159997, 0.44959998, 0.55759996],\n        [0.4604    , 0.61239994, 0.76439995],\n        [0.57919997, 0.77519995, 0.9711999 ],\n        [0.698     , 0.938     , 1.178     ]],\n\n       [[0.55759996, 0.7456    , 0.93359995],\n        [0.67639995, 0.90839994, 1.1403999 ],\n        [0.36319998, 0.4792    , 0.5952    ],\n        [0.482     , 0.64199996, 0.802     ]]], dtype=float32)>, <tf.Tensor: id=2192, shape=(2, 4, 3), dtype=float32, numpy=\narray([[[0.6656   , 0.7736   , 0.8816   ],\n        [0.9164   , 1.0684   , 1.2204   ],\n        [1.1672   , 1.3632   , 1.5591999],\n        [1.418    , 1.658    , 1.8980001]],\n\n       [[1.1216   , 1.3096   , 1.4976001],\n        [1.3723999, 1.6043999, 1.8364   ],\n        [0.7112   , 0.8272   , 0.9432   ],\n        [0.962    , 1.122    , 1.282    ]]], dtype=float32)>]\nencoder_Q_concat:\ntf.Tensor(\n[[[0.32       0.428      0.536     ]\n  [0.43       0.582      0.734     ]\n  [0.54       0.73599994 0.932     ]\n  [0.65000004 0.89       1.1300001 ]]\n\n [[0.52       0.708      0.896     ]\n  [0.63       0.862      1.094     ]\n  [0.34       0.456      0.572     ]\n  [0.45000002 0.61       0.77000004]]\n\n [[0.644      0.752      0.85999995]\n  [0.886      1.038      1.1899999 ]\n  [1.128      1.324      1.52      ]\n  [1.37       1.61       1.85      ]]\n\n [[1.084      1.272      1.46      ]\n  [1.326      1.558      1.79      ]\n  [0.68799996 0.804      0.91999996]\n  [0.93       1.09       1.25      ]]], shape=(4, 4, 3), dtype=float32)\nencoder_K_concat:\ntf.Tensor(\n[[[0.29839998 0.4064     0.5144    ]\n  [0.3996     0.5516     0.7036    ]\n  [0.5008     0.6968     0.8927999 ]\n  [0.602      0.842      1.082     ]]\n\n [[0.4824     0.67039996 0.8584    ]\n  [0.5836     0.8156     1.0475999 ]\n  [0.3168     0.4328     0.5488    ]\n  [0.418      0.578      0.738     ]]\n\n [[0.6224     0.73039997 0.83839995]\n  [0.8556     1.0076     1.1596    ]\n  [1.0888     1.2847999  1.4807999 ]\n  [1.322      1.562      1.8019999 ]]\n\n [[1.0464     1.2343999  1.4224    ]\n  [1.2795999  1.5115999  1.7435999 ]\n  [0.6648     0.7808     0.8968    ]\n  [0.898      1.058      1.2179999 ]]], shape=(4, 4, 3), dtype=float32)\nencoder_V_concat:\ntf.Tensor(\n[[[0.34159997 0.44959998 0.55759996]\n  [0.4604     0.61239994 0.76439995]\n  [0.57919997 0.77519995 0.9711999 ]\n  [0.698      0.938      1.178     ]]\n\n [[0.55759996 0.7456     0.93359995]\n  [0.67639995 0.90839994 1.1403999 ]\n  [0.36319998 0.4792     0.5952    ]\n  [0.482      0.64199996 0.802     ]]\n\n [[0.6656     0.7736     0.8816    ]\n  [0.9164     1.0684     1.2204    ]\n  [1.1672     1.3632     1.5591999 ]\n  [1.418      1.658      1.8980001 ]]\n\n [[1.1216     1.3096     1.4976001 ]\n  [1.3723999  1.6043999  1.8364    ]\n  [0.7112     0.8272     0.9432    ]\n  [0.962      1.122      1.282     ]]], shape=(4, 4, 3), dtype=float32)\nattention_map\ntf.Tensor(\n[[[0.24089162 0.24686453 0.25298554 0.25925833]\n  [0.23763184 0.2456934  0.2540285  0.26264632]\n  [0.23439312 0.24450381 0.25505063 0.2660524 ]\n  [0.23117585 0.2432961  0.25605178 0.26947623]]\n\n [[0.25306    0.26355058 0.23678671 0.24660268]\n  [0.253682   0.266552   0.23394856 0.24581744]\n  [0.25200623 0.25867152 0.24146782 0.2478544 ]\n  [0.2526555  0.26164833 0.23860176 0.24709438]]\n\n [[0.2210725  0.2393314  0.2590983  0.28049782]\n  [0.21052654 0.23490563 0.2621078  0.29246005]\n  [0.20025736 0.23030145 0.264853   0.30458823]\n  [0.19027545 0.22553429 0.2673267  0.31686354]]\n\n [[0.25876325 0.29595453 0.20771402 0.2375682 ]\n  [0.26018527 0.3067092  0.19878069 0.23432481]\n  [0.25599337 0.27866307 0.2228064  0.24253717]\n  [0.25775087 0.2891828  0.21351446 0.23955189]]], shape=(4, 4, 4), dtype=float32)\nweightedSumV:\ntf.Tensor(\n[[[0.52343655 0.69878334 0.87413025]\n  [0.5247527  0.70058703 0.87642133]\n  [0.52606815 0.70238966 0.8787111 ]\n  [0.5273828  0.7041912  0.8809997 ]]\n\n [[0.52423525 0.699878   0.87552065]\n  [0.52520293 0.70120406 0.87720513]\n  [0.522651   0.69770694 0.87276286]\n  [0.52361923 0.69903386 0.87444836]]\n\n [[1.0666345  1.2449915  1.4233485 ]\n  [1.0760345  1.2560406  1.4360467 ]\n  [1.085382   1.2670281  1.4486741 ]\n  [1.0946631  1.2779374  1.4612117 ]]\n\n [[1.0726637  1.2520784  1.431493  ]\n  [1.0795448  1.2601666  1.4407886 ]\n  [1.06134    1.2387681  1.4161961 ]\n  [1.0682683  1.2469119  1.4255555 ]]], shape=(4, 4, 3), dtype=float32)\noutputs_z:\ntf.Tensor(\n[[[0.52343655 0.69878334 0.87413025 1.0666345  1.2449915  1.4233485 ]\n  [0.5247527  0.70058703 0.87642133 1.0760345  1.2560406  1.4360467 ]\n  [0.52606815 0.70238966 0.8787111  1.085382   1.2670281  1.4486741 ]\n  [0.5273828  0.7041912  0.8809997  1.0946631  1.2779374  1.4612117 ]]\n\n [[0.52423525 0.699878   0.87552065 1.0726637  1.2520784  1.431493  ]\n  [0.52520293 0.70120406 0.87720513 1.0795448  1.2601666  1.4407886 ]\n  [0.522651   0.69770694 0.87276286 1.06134    1.2387681  1.4161961 ]\n  [0.52361923 0.69903386 0.87444836 1.0682683  1.2469119  1.4255555 ]]], shape=(2, 4, 6), dtype=float32)\noutputs:\ntf.Tensor(\n[[[0.5831325  1.166265   1.7493975  2.33253   ]\n  [0.58698833 1.1739767  1.7609649  2.3479533 ]\n  [0.5908253  1.1816506  1.7724761  2.3633013 ]\n  [0.5946386  1.1892772  1.7839159  2.3785543 ]]\n\n [[0.5855869  1.1711738  1.7567607  2.3423476 ]\n  [0.5884112  1.1768224  1.7652336  2.3536448 ]\n  [0.5809425  1.161885   1.7428275  2.32377   ]\n  [0.58378375 1.1675675  1.7513512  2.335135  ]]], shape=(2, 4, 4), dtype=float32)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "print('Multi-Head Attention:')\n",
    "w_Z = tf.constant([[0.1,0.2,0.3,0.4],\n",
    "                   [0.1,0.2,0.3,0.4],\n",
    "                   [0.1,0.2,0.3,0.4],\n",
    "                   [0.1,0.2,0.3,0.4],\n",
    "                   [0.1,0.2,0.3,0.4],\n",
    "                   [0.1,0.2,0.3,0.4]], dtype=tf.float32)\n",
    "\n",
    "with tf.variable_scope('encoder_input'):\n",
    "    encoder_embedding_input = tf.nn.embedding_lookup(chinese_embedding, encoder_input)\n",
    "    encoder_embedding_input += position_embedding\n",
    "    \n",
    "with tf.variable_scope('encoder_multi_head_product_attention'):\n",
    "    encoder_Q = tf.matmul(tf.reshape(encoder_embedding_input, (-1, tf.shape(encoder_embedding_input)[2])), encoder_w_Q)\n",
    "    encoder_K = tf.matmul(tf.reshape(encoder_embedding_input, (-1, tf.shape(encoder_embedding_input)[2])), encoder_w_K)\n",
    "    encoder_V = tf.matmul(tf.reshape(encoder_embedding_input, (-1, tf.shape(encoder_embedding_input)[2])), encoder_w_V)\n",
    "    \n",
    "    print('encoder_Q:')\n",
    "    print(encoder_Q)\n",
    "    print('encoder_K:')\n",
    "    print(encoder_K)\n",
    "    print('encoder_V:')\n",
    "    print(encoder_V)\n",
    "    \n",
    "    encoder_Q = tf.reshape(encoder_Q, (tf.shape(encoder_embedding_input)[0], tf.shape(encoder_embedding_input)[1], -1))\n",
    "    encoder_K = tf.reshape(encoder_K, (tf.shape(encoder_embedding_input)[0], tf.shape(encoder_embedding_input)[1], -1))\n",
    "    encoder_V = tf.reshape(encoder_V, (tf.shape(encoder_embedding_input)[0], tf.shape(encoder_embedding_input)[1], -1))\n",
    "    \n",
    "    print()\n",
    "    print('after reshape:')\n",
    "    print('encoder_Q:')\n",
    "    print(encoder_Q)\n",
    "    print('encoder_K:')\n",
    "    print(encoder_K)\n",
    "    print('encoder_V:')\n",
    "    print(encoder_V)\n",
    "    \n",
    "    encoder_Q_split = tf.split(encoder_Q, 2, axis=2)\n",
    "    encoder_K_split = tf.split(encoder_K, 2, axis=2)\n",
    "    encoder_V_split = tf.split(encoder_V, 2, axis=2)\n",
    "    \n",
    "    print('encoder_Q_split:')\n",
    "    print(encoder_Q_split)\n",
    "    print('encoder_K_split:')\n",
    "    print(encoder_K_split)\n",
    "    print('encoder_V_split:')\n",
    "    print(encoder_V_split)\n",
    "    \n",
    "    encoder_Q_concat = tf.concat(encoder_Q_split, axis=0)\n",
    "    encoder_K_concat = tf.concat(encoder_K_split, axis=0)\n",
    "    encoder_V_concat = tf.concat(encoder_V_split, axis=0)\n",
    "    \n",
    "    print('encoder_Q_concat:')\n",
    "    print(encoder_Q_concat)\n",
    "    print('encoder_K_concat:')\n",
    "    print(encoder_K_concat)\n",
    "    print('encoder_V_concat:')\n",
    "    print(encoder_V_concat)\n",
    "    \n",
    "    attention_map = tf.matmul(encoder_Q_concat, tf.transpose(encoder_K_concat, [0, 2, 1]))\n",
    "    attention_map /= 8\n",
    "    attention_map = tf.nn.softmax(attention_map)\n",
    "    \n",
    "    print('attention_map')\n",
    "    print(attention_map)\n",
    "    \n",
    "    weightedSumV = tf.matmul(attention_map, encoder_V_concat)\n",
    "    \n",
    "    print('weightedSumV:')\n",
    "    print(weightedSumV)\n",
    "    \n",
    "    outputs_z = tf.concat(tf.split(weightedSumV, 2, axis=0), axis=2)\n",
    "    \n",
    "    print('outputs_z:')\n",
    "    print(outputs_z)\n",
    "    \n",
    "    outputs = tf.matmul(tf.reshape(outputs_z, (-1, tf.shape(outputs_z)[2])), w_Z)\n",
    "    outputs = tf.reshape(outputs, (tf.shape(encoder_embedding_input)[0], tf.shape(encoder_embedding_input)[1], -1))\n",
    "    \n",
    "    print('outputs:')\n",
    "    print(outputs)\n",
    "    "
   ],
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "encoder_Q:\ntf.Tensor(\n[[0.32       0.428      0.536      0.644      0.752      0.85999995]\n [0.43       0.582      0.734      0.886      1.038      1.1899999 ]\n [0.54       0.73599994 0.932      1.128      1.324      1.52      ]\n [0.65000004 0.89       1.1300001  1.37       1.61       1.85      ]\n [0.52       0.708      0.896      1.084      1.272      1.46      ]\n [0.63       0.862      1.094      1.326      1.558      1.79      ]\n [0.34       0.456      0.572      0.68799996 0.804      0.91999996]\n [0.45000002 0.61       0.77000004 0.93       1.09       1.25      ]], shape=(8, 6), dtype=float32)\nencoder_K:\ntf.Tensor(\n[[0.29839998 0.4064     0.5144     0.6224     0.73039997 0.83839995]\n [0.3996     0.5516     0.7036     0.8556     1.0076     1.1596    ]\n [0.5008     0.6968     0.8927999  1.0888     1.2847999  1.4807999 ]\n [0.602      0.842      1.082      1.322      1.562      1.8019999 ]\n [0.4824     0.67039996 0.8584     1.0464     1.2343999  1.4224    ]\n [0.5836     0.8156     1.0475999  1.2795999  1.5115999  1.7435999 ]\n [0.3168     0.4328     0.5488     0.6648     0.7808     0.8968    ]\n [0.418      0.578      0.738      0.898      1.058      1.2179999 ]], shape=(8, 6), dtype=float32)\nencoder_V:\ntf.Tensor(\n[[0.34159997 0.44959998 0.55759996 0.6656     0.7736     0.8816    ]\n [0.4604     0.61239994 0.76439995 0.9164     1.0684     1.2204    ]\n [0.57919997 0.77519995 0.9711999  1.1672     1.3632     1.5591999 ]\n [0.698      0.938      1.178      1.418      1.658      1.8980001 ]\n [0.55759996 0.7456     0.93359995 1.1216     1.3096     1.4976001 ]\n [0.67639995 0.90839994 1.1403999  1.3723999  1.6043999  1.8364    ]\n [0.36319998 0.4792     0.5952     0.7112     0.8272     0.9432    ]\n [0.482      0.64199996 0.802      0.962      1.122      1.282     ]], shape=(8, 6), dtype=float32)\n\nafter reshape:\nencoder_Q:\ntf.Tensor(\n[[[0.32       0.428      0.536      0.644      0.752      0.85999995]\n  [0.43       0.582      0.734      0.886      1.038      1.1899999 ]\n  [0.54       0.73599994 0.932      1.128      1.324      1.52      ]\n  [0.65000004 0.89       1.1300001  1.37       1.61       1.85      ]]\n\n [[0.52       0.708      0.896      1.084      1.272      1.46      ]\n  [0.63       0.862      1.094      1.326      1.558      1.79      ]\n  [0.34       0.456      0.572      0.68799996 0.804      0.91999996]\n  [0.45000002 0.61       0.77000004 0.93       1.09       1.25      ]]], shape=(2, 4, 6), dtype=float32)\nencoder_K:\ntf.Tensor(\n[[[0.29839998 0.4064     0.5144     0.6224     0.73039997 0.83839995]\n  [0.3996     0.5516     0.7036     0.8556     1.0076     1.1596    ]\n  [0.5008     0.6968     0.8927999  1.0888     1.2847999  1.4807999 ]\n  [0.602      0.842      1.082      1.322      1.562      1.8019999 ]]\n\n [[0.4824     0.67039996 0.8584     1.0464     1.2343999  1.4224    ]\n  [0.5836     0.8156     1.0475999  1.2795999  1.5115999  1.7435999 ]\n  [0.3168     0.4328     0.5488     0.6648     0.7808     0.8968    ]\n  [0.418      0.578      0.738      0.898      1.058      1.2179999 ]]], shape=(2, 4, 6), dtype=float32)\nencoder_V:\ntf.Tensor(\n[[[0.34159997 0.44959998 0.55759996 0.6656     0.7736     0.8816    ]\n  [0.4604     0.61239994 0.76439995 0.9164     1.0684     1.2204    ]\n  [0.57919997 0.77519995 0.9711999  1.1672     1.3632     1.5591999 ]\n  [0.698      0.938      1.178      1.418      1.658      1.8980001 ]]\n\n [[0.55759996 0.7456     0.93359995 1.1216     1.3096     1.4976001 ]\n  [0.67639995 0.90839994 1.1403999  1.3723999  1.6043999  1.8364    ]\n  [0.36319998 0.4792     0.5952     0.7112     0.8272     0.9432    ]\n  [0.482      0.64199996 0.802      0.962      1.122      1.282     ]]], shape=(2, 4, 6), dtype=float32)\nencoder_Q_split:\n[<tf.Tensor: id=2373, shape=(2, 4, 3), dtype=float32, numpy=\narray([[[0.32      , 0.428     , 0.536     ],\n        [0.43      , 0.582     , 0.734     ],\n        [0.54      , 0.73599994, 0.932     ],\n        [0.65000004, 0.89      , 1.1300001 ]],\n\n       [[0.52      , 0.708     , 0.896     ],\n        [0.63      , 0.862     , 1.094     ],\n        [0.34      , 0.456     , 0.572     ],\n        [0.45000002, 0.61      , 0.77000004]]], dtype=float32)>, <tf.Tensor: id=2374, shape=(2, 4, 3), dtype=float32, numpy=\narray([[[0.644     , 0.752     , 0.85999995],\n        [0.886     , 1.038     , 1.1899999 ],\n        [1.128     , 1.324     , 1.52      ],\n        [1.37      , 1.61      , 1.85      ]],\n\n       [[1.084     , 1.272     , 1.46      ],\n        [1.326     , 1.558     , 1.79      ],\n        [0.68799996, 0.804     , 0.91999996],\n        [0.93      , 1.09      , 1.25      ]]], dtype=float32)>]\nencoder_K_split:\n[<tf.Tensor: id=2377, shape=(2, 4, 3), dtype=float32, numpy=\narray([[[0.29839998, 0.4064    , 0.5144    ],\n        [0.3996    , 0.5516    , 0.7036    ],\n        [0.5008    , 0.6968    , 0.8927999 ],\n        [0.602     , 0.842     , 1.082     ]],\n\n       [[0.4824    , 0.67039996, 0.8584    ],\n        [0.5836    , 0.8156    , 1.0475999 ],\n        [0.3168    , 0.4328    , 0.5488    ],\n        [0.418     , 0.578     , 0.738     ]]], dtype=float32)>, <tf.Tensor: id=2378, shape=(2, 4, 3), dtype=float32, numpy=\narray([[[0.6224    , 0.73039997, 0.83839995],\n        [0.8556    , 1.0076    , 1.1596    ],\n        [1.0888    , 1.2847999 , 1.4807999 ],\n        [1.322     , 1.562     , 1.8019999 ]],\n\n       [[1.0464    , 1.2343999 , 1.4224    ],\n        [1.2795999 , 1.5115999 , 1.7435999 ],\n        [0.6648    , 0.7808    , 0.8968    ],\n        [0.898     , 1.058     , 1.2179999 ]]], dtype=float32)>]\nencoder_V_split:\n[<tf.Tensor: id=2381, shape=(2, 4, 3), dtype=float32, numpy=\narray([[[0.34159997, 0.44959998, 0.55759996],\n        [0.4604    , 0.61239994, 0.76439995],\n        [0.57919997, 0.77519995, 0.9711999 ],\n        [0.698     , 0.938     , 1.178     ]],\n\n       [[0.55759996, 0.7456    , 0.93359995],\n        [0.67639995, 0.90839994, 1.1403999 ],\n        [0.36319998, 0.4792    , 0.5952    ],\n        [0.482     , 0.64199996, 0.802     ]]], dtype=float32)>, <tf.Tensor: id=2382, shape=(2, 4, 3), dtype=float32, numpy=\narray([[[0.6656   , 0.7736   , 0.8816   ],\n        [0.9164   , 1.0684   , 1.2204   ],\n        [1.1672   , 1.3632   , 1.5591999],\n        [1.418    , 1.658    , 1.8980001]],\n\n       [[1.1216   , 1.3096   , 1.4976001],\n        [1.3723999, 1.6043999, 1.8364   ],\n        [0.7112   , 0.8272   , 0.9432   ],\n        [0.962    , 1.122    , 1.282    ]]], dtype=float32)>]\nencoder_Q_concat:\ntf.Tensor(\n[[[0.32       0.428      0.536     ]\n  [0.43       0.582      0.734     ]\n  [0.54       0.73599994 0.932     ]\n  [0.65000004 0.89       1.1300001 ]]\n\n [[0.52       0.708      0.896     ]\n  [0.63       0.862      1.094     ]\n  [0.34       0.456      0.572     ]\n  [0.45000002 0.61       0.77000004]]\n\n [[0.644      0.752      0.85999995]\n  [0.886      1.038      1.1899999 ]\n  [1.128      1.324      1.52      ]\n  [1.37       1.61       1.85      ]]\n\n [[1.084      1.272      1.46      ]\n  [1.326      1.558      1.79      ]\n  [0.68799996 0.804      0.91999996]\n  [0.93       1.09       1.25      ]]], shape=(4, 4, 3), dtype=float32)\nencoder_K_concat:\ntf.Tensor(\n[[[0.29839998 0.4064     0.5144    ]\n  [0.3996     0.5516     0.7036    ]\n  [0.5008     0.6968     0.8927999 ]\n  [0.602      0.842      1.082     ]]\n\n [[0.4824     0.67039996 0.8584    ]\n  [0.5836     0.8156     1.0475999 ]\n  [0.3168     0.4328     0.5488    ]\n  [0.418      0.578      0.738     ]]\n\n [[0.6224     0.73039997 0.83839995]\n  [0.8556     1.0076     1.1596    ]\n  [1.0888     1.2847999  1.4807999 ]\n  [1.322      1.562      1.8019999 ]]\n\n [[1.0464     1.2343999  1.4224    ]\n  [1.2795999  1.5115999  1.7435999 ]\n  [0.6648     0.7808     0.8968    ]\n  [0.898      1.058      1.2179999 ]]], shape=(4, 4, 3), dtype=float32)\nencoder_V_concat:\ntf.Tensor(\n[[[0.34159997 0.44959998 0.55759996]\n  [0.4604     0.61239994 0.76439995]\n  [0.57919997 0.77519995 0.9711999 ]\n  [0.698      0.938      1.178     ]]\n\n [[0.55759996 0.7456     0.93359995]\n  [0.67639995 0.90839994 1.1403999 ]\n  [0.36319998 0.4792     0.5952    ]\n  [0.482      0.64199996 0.802     ]]\n\n [[0.6656     0.7736     0.8816    ]\n  [0.9164     1.0684     1.2204    ]\n  [1.1672     1.3632     1.5591999 ]\n  [1.418      1.658      1.8980001 ]]\n\n [[1.1216     1.3096     1.4976001 ]\n  [1.3723999  1.6043999  1.8364    ]\n  [0.7112     0.8272     0.9432    ]\n  [0.962      1.122      1.282     ]]], shape=(4, 4, 3), dtype=float32)\nattention_map\ntf.Tensor(\n[[[0.24089162 0.24686453 0.25298554 0.25925833]\n  [0.23763184 0.2456934  0.2540285  0.26264632]\n  [0.23439312 0.24450381 0.25505063 0.2660524 ]\n  [0.23117585 0.2432961  0.25605178 0.26947623]]\n\n [[0.25306    0.26355058 0.23678671 0.24660268]\n  [0.253682   0.266552   0.23394856 0.24581744]\n  [0.25200623 0.25867152 0.24146782 0.2478544 ]\n  [0.2526555  0.26164833 0.23860176 0.24709438]]\n\n [[0.2210725  0.2393314  0.2590983  0.28049782]\n  [0.21052654 0.23490563 0.2621078  0.29246005]\n  [0.20025736 0.23030145 0.264853   0.30458823]\n  [0.19027545 0.22553429 0.2673267  0.31686354]]\n\n [[0.25876325 0.29595453 0.20771402 0.2375682 ]\n  [0.26018527 0.3067092  0.19878069 0.23432481]\n  [0.25599337 0.27866307 0.2228064  0.24253717]\n  [0.25775087 0.2891828  0.21351446 0.23955189]]], shape=(4, 4, 4), dtype=float32)\nweightedSumV:\ntf.Tensor(\n[[[0.52343655 0.69878334 0.87413025]\n  [0.5247527  0.70058703 0.87642133]\n  [0.52606815 0.70238966 0.8787111 ]\n  [0.5273828  0.7041912  0.8809997 ]]\n\n [[0.52423525 0.699878   0.87552065]\n  [0.52520293 0.70120406 0.87720513]\n  [0.522651   0.69770694 0.87276286]\n  [0.52361923 0.69903386 0.87444836]]\n\n [[1.0666345  1.2449915  1.4233485 ]\n  [1.0760345  1.2560406  1.4360467 ]\n  [1.085382   1.2670281  1.4486741 ]\n  [1.0946631  1.2779374  1.4612117 ]]\n\n [[1.0726637  1.2520784  1.431493  ]\n  [1.0795448  1.2601666  1.4407886 ]\n  [1.06134    1.2387681  1.4161961 ]\n  [1.0682683  1.2469119  1.4255555 ]]], shape=(4, 4, 3), dtype=float32)\noutputs_z:\ntf.Tensor(\n[[[0.52343655 0.69878334 0.87413025 1.0666345  1.2449915  1.4233485 ]\n  [0.5247527  0.70058703 0.87642133 1.0760345  1.2560406  1.4360467 ]\n  [0.52606815 0.70238966 0.8787111  1.085382   1.2670281  1.4486741 ]\n  [0.5273828  0.7041912  0.8809997  1.0946631  1.2779374  1.4612117 ]]\n\n [[0.52423525 0.699878   0.87552065 1.0726637  1.2520784  1.431493  ]\n  [0.52520293 0.70120406 0.87720513 1.0795448  1.2601666  1.4407886 ]\n  [0.522651   0.69770694 0.87276286 1.06134    1.2387681  1.4161961 ]\n  [0.52361923 0.69903386 0.87444836 1.0682683  1.2469119  1.4255555 ]]], shape=(2, 4, 6), dtype=float32)\noutputs:\ntf.Tensor(\n[[[0.5831325  1.166265   1.7493975  2.33253   ]\n  [0.58698833 1.1739767  1.7609649  2.3479533 ]\n  [0.5908253  1.1816506  1.7724761  2.3633013 ]\n  [0.5946386  1.1892772  1.7839159  2.3785543 ]]\n\n [[0.5855869  1.1711738  1.7567607  2.3423476 ]\n  [0.5884112  1.1768224  1.7652336  2.3536448 ]\n  [0.5809425  1.161885   1.7428275  2.32377   ]\n  [0.58378375 1.1675675  1.7513512  2.335135  ]]], shape=(2, 4, 4), dtype=float32)\n\nadd outputs:\ntf.Tensor(\n[[[0.7031325  1.386265   2.0693974  2.75253   ]\n  [0.81698835 1.5039767  2.190965   2.8779533 ]\n  [0.93082535 1.6216507  2.3124762  3.0033011 ]\n  [1.0446386  1.7392771  2.4339159  3.1285543 ]]\n\n [[0.9055869  1.5911738  2.2767606  2.9623475 ]\n  [1.0184112  1.7068224  2.3952336  3.0836449 ]\n  [0.7209425  1.401885   2.0828276  2.76377   ]\n  [0.83378375 1.5175675  2.2013512  2.885135  ]]], shape=(2, 4, 4), dtype=float32)\nadd encoder_outputs:\ntf.Tensor(\n[[[2.0853977 3.4596627 5.5250597 5.5170603]\n  [2.294965  3.7209415 5.885906  5.8339067]\n  [2.504476  3.982127  6.246603  6.1506023]\n  [2.7139158 4.243193  6.607109  6.4671087]]\n\n [[2.4527607 3.9119344 6.1446953 6.056695 ]\n  [2.6592336 4.168056  6.4972897 6.3652897]\n  [2.1148276 3.4927127 5.56754   5.5515404]\n  [2.321351  3.748919  5.92027   5.8602695]]], shape=(2, 4, 4), dtype=float32)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "\n",
    "with tf.variable_scope('encoder_block'):\n",
    "    encoder_Q = tf.matmul(tf.reshape(encoder_embedding_input, (-1, tf.shape(encoder_embedding_input)[2])), encoder_w_Q)\n",
    "    encoder_K = tf.matmul(tf.reshape(encoder_embedding_input, (-1, tf.shape(encoder_embedding_input)[2])), encoder_w_K)\n",
    "    encoder_V = tf.matmul(tf.reshape(encoder_embedding_input, (-1, tf.shape(encoder_embedding_input)[2])), encoder_w_V)\n",
    "    \n",
    "    print('encoder_Q:')\n",
    "    print(encoder_Q)\n",
    "    print('encoder_K:')\n",
    "    print(encoder_K)\n",
    "    print('encoder_V:')\n",
    "    print(encoder_V)\n",
    "    \n",
    "    encoder_Q = tf.reshape(encoder_Q, (tf.shape(encoder_embedding_input)[0], tf.shape(encoder_embedding_input)[1], -1))\n",
    "    encoder_K = tf.reshape(encoder_K, (tf.shape(encoder_embedding_input)[0], tf.shape(encoder_embedding_input)[1], -1))\n",
    "    encoder_V = tf.reshape(encoder_V, (tf.shape(encoder_embedding_input)[0], tf.shape(encoder_embedding_input)[1], -1))\n",
    "    \n",
    "    print()\n",
    "    print('after reshape:')\n",
    "    print('encoder_Q:')\n",
    "    print(encoder_Q)\n",
    "    print('encoder_K:')\n",
    "    print(encoder_K)\n",
    "    print('encoder_V:')\n",
    "    print(encoder_V)\n",
    "    \n",
    "    encoder_Q_split = tf.split(encoder_Q, 2, axis=2)\n",
    "    encoder_K_split = tf.split(encoder_K, 2, axis=2)\n",
    "    encoder_V_split = tf.split(encoder_V, 2, axis=2)\n",
    "    \n",
    "    print('encoder_Q_split:')\n",
    "    print(encoder_Q_split)\n",
    "    print('encoder_K_split:')\n",
    "    print(encoder_K_split)\n",
    "    print('encoder_V_split:')\n",
    "    print(encoder_V_split)\n",
    "    \n",
    "    encoder_Q_concat = tf.concat(encoder_Q_split, axis=0)\n",
    "    encoder_K_concat = tf.concat(encoder_K_split, axis=0)\n",
    "    encoder_V_concat = tf.concat(encoder_V_split, axis=0)\n",
    "    \n",
    "    print('encoder_Q_concat:')\n",
    "    print(encoder_Q_concat)\n",
    "    print('encoder_K_concat:')\n",
    "    print(encoder_K_concat)\n",
    "    print('encoder_V_concat:')\n",
    "    print(encoder_V_concat)\n",
    "    \n",
    "    attention_map = tf.matmul(encoder_Q_concat, tf.transpose(encoder_K_concat, [0, 2, 1]))\n",
    "    attention_map /= 8\n",
    "    attention_map = tf.nn.softmax(attention_map)\n",
    "    \n",
    "    print('attention_map')\n",
    "    print(attention_map)\n",
    "    \n",
    "    weightedSumV = tf.matmul(attention_map, encoder_V_concat)\n",
    "    \n",
    "    print('weightedSumV:')\n",
    "    print(weightedSumV)\n",
    "    \n",
    "    outputs_z = tf.concat(tf.split(weightedSumV, 2, axis=0), axis=2)\n",
    "    \n",
    "    print('outputs_z:')\n",
    "    print(outputs_z)\n",
    "    \n",
    "    outputs = tf.matmul(tf.reshape(outputs_z, (-1, tf.shape(outputs_z)[2])), w_Z)\n",
    "    outputs = tf.reshape(outputs, (tf.shape(encoder_embedding_input)[0], tf.shape(encoder_embedding_input)[1], -1))\n",
    "    \n",
    "    print('outputs:')\n",
    "    print(outputs)\n",
    "    \n",
    "    outputs += encoder_embedding_input\n",
    "    \n",
    "    print()\n",
    "    print('add outputs:')\n",
    "    print(outputs)\n",
    "    \n",
    "    W_f = tf.constant([[0.2,0.3,0.5,0.4],\n",
    "                       [0.2,0.3,0.5,0.4],\n",
    "                       [0.2,0.3,0.5,0.4],\n",
    "                       [0.2,0.3,0.5,0.4]])\n",
    "    \n",
    "    ffn_outputs = tf.matmul(tf.reshape(outputs, (-1, tf.shape(outputs)[2])), W_f)\n",
    "    ffn_outputs = tf.reshape(ffn_outputs, (tf.shape(outputs)[0], tf.shape(outputs)[1], -1))\n",
    "    \n",
    "    encoder_outputs = ffn_outputs + outputs\n",
    "    \n",
    "    print('add encoder_outputs:')\n",
    "    print(encoder_outputs)\n",
    "    "
   ],
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Decoder Block:\ntf.Tensor(\n[[[0.61 0.71 0.81 0.91]\n  [0.71 0.81 0.91 1.01]]\n\n [[0.71 0.81 0.91 1.01]\n  [0.61 0.71 0.81 0.91]]], shape=(2, 2, 4), dtype=float32)\ndecoder_embedding_input:\ntf.Tensor(\n[[[0.62       0.71999997 0.82       0.92      ]\n  [0.72999996 0.83       0.93       1.03      ]]\n\n [[0.71999997 0.82       0.92       1.02      ]\n  [0.63       0.72999996 0.83       0.93      ]]], shape=(2, 2, 4), dtype=float32)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "\n",
    "print('Decoder Block:')\n",
    "\n",
    "english_embedding = tf.constant([[0.51,0.61,0.71,0.81],\n",
    "                         [0.61,0.71,0.81,0.91],\n",
    "                         [0.71,0.81,0.91,1.01],\n",
    "                         [0.81,0.91,1.01,1.11]],dtype=tf.float32)\n",
    "\n",
    "decoder_input = tf.constant([[1, 2], [2, 1]], dtype=tf.int32)\n",
    "\n",
    "with tf.variable_scope('decoder_input'):\n",
    "    decoder_embedding_input = tf.nn.embedding_lookup(english_embedding, decoder_input)\n",
    "    print(decoder_embedding_input)\n",
    "    decoder_embedding_input += position_embedding[0:tf.shape(decoder_embedding_input)[1]]\n",
    "    \n",
    "    print('decoder_embedding_input:')\n",
    "    print(decoder_embedding_input)\n",
    "    "
   ],
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "decoder self-attention:\nbefore mask: tf.Tensor(\n[[[0.61042327 0.69419265]\n  [0.69435763 0.78964627]]\n\n [[0.7724013  0.6952947 ]\n  [0.6951597  0.62576425]]\n\n [[1.7976708  2.0485718 ]\n  [2.048737   2.3346798 ]]\n\n [[2.2829647  2.051618  ]\n  [2.0514832  1.8435937 ]]], shape=(4, 2, 2), dtype=float32)\ndiag_vals tf.Tensor(\n[[1. 1.]\n [1. 1.]], shape=(2, 2), dtype=float32)\ntril tf.Tensor(\n[[1. 0.]\n [1. 1.]], shape=(2, 2), dtype=float32)\nmasks tf.Tensor(\n[[[1. 0.]\n  [1. 1.]]\n\n [[1. 0.]\n  [1. 1.]]\n\n [[1. 0.]\n  [1. 1.]]\n\n [[1. 0.]\n  [1. 1.]]], shape=(4, 2, 2), dtype=float32)\npaddings tf.Tensor(\n[[[-4.2949673e+09 -4.2949673e+09]\n  [-4.2949673e+09 -4.2949673e+09]]\n\n [[-4.2949673e+09 -4.2949673e+09]\n  [-4.2949673e+09 -4.2949673e+09]]\n\n [[-4.2949673e+09 -4.2949673e+09]\n  [-4.2949673e+09 -4.2949673e+09]]\n\n [[-4.2949673e+09 -4.2949673e+09]\n  [-4.2949673e+09 -4.2949673e+09]]], shape=(4, 2, 2), dtype=float32)\ndecoder_sa_attention_map tf.Tensor(\n[[[ 6.1042327e-01 -4.2949673e+09]\n  [ 6.9435763e-01  7.8964627e-01]]\n\n [[ 7.7240127e-01 -4.2949673e+09]\n  [ 6.9515967e-01  6.2576425e-01]]\n\n [[ 1.7976708e+00 -4.2949673e+09]\n  [ 2.0487370e+00  2.3346798e+00]]\n\n [[ 2.2829647e+00 -4.2949673e+09]\n  [ 2.0514832e+00  1.8435937e+00]]], shape=(4, 2, 2), dtype=float32)\ndecoder_sa_attention_map:\ntf.Tensor(\n[[[1.         0.        ]\n  [0.47619584 0.5238041 ]]\n\n [[1.         0.        ]\n  [0.5173419  0.4826581 ]]\n\n [[1.         0.        ]\n  [0.42899743 0.5710026 ]]\n\n [[1.         0.        ]\n  [0.551786   0.448214  ]]], shape=(4, 2, 2), dtype=float32)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "\n",
    "print('decoder self-attention:')\n",
    "\n",
    "w_Q_decoder_sa = tf.constant([[0.15,0.25,0.35,0.45,0.55,0.65],\n",
    "                              [0.25,0.35,0.45,0.55,0.65,0.75],\n",
    "                              [0.35,0.45,0.55,0.65,0.75,0.85],\n",
    "                              [0.45,0.55,0.65,0.75,0.85,0.95]], dtype=tf.float32)\n",
    "\n",
    "w_K_decoder_sa = tf.constant([[0.13,0.23,0.33,0.43,0.53,0.63],\n",
    "                              [0.23,0.33,0.43,0.53,0.63,0.73],\n",
    "                              [0.33,0.43,0.53,0.63,0.73,0.83],\n",
    "                              [0.43,0.53,0.63,0.73,0.83,0.93]], dtype=tf.float32)\n",
    "\n",
    "w_V_decoder_sa = tf.constant([[0.17,0.27,0.37,0.47,0.57,0.67],\n",
    "                              [0.27,0.37,0.47,0.57,0.67,0.77],\n",
    "                              [0.37,0.47,0.57,0.67,0.77,0.87],\n",
    "                              [0.47,0.57,0.67,0.77,0.87,0.97]], dtype=tf.float32)\n",
    "\n",
    "with tf.variable_scope(\"decoder_sa_block\"):\n",
    "    decoder_Q = tf.matmul(tf.reshape(decoder_embedding_input,(-1, tf.shape(decoder_embedding_input)[2])), w_Q_decoder_sa)\n",
    "    decoder_K = tf.matmul(tf.reshape(decoder_embedding_input,(-1, tf.shape(decoder_embedding_input)[2])), w_K_decoder_sa)\n",
    "    decoder_V = tf.matmul(tf.reshape(decoder_embedding_input,(-1, tf.shape(decoder_embedding_input)[2])), w_V_decoder_sa)\n",
    "    \n",
    "    decoder_Q = tf.reshape(decoder_Q, (tf.shape(decoder_embedding_input)[0], tf.shape(decoder_embedding_input)[1], -1))\n",
    "    decoder_K = tf.reshape(decoder_K, (tf.shape(decoder_embedding_input)[0], tf.shape(decoder_embedding_input)[1], -1))\n",
    "    decoder_V = tf.reshape(decoder_V, (tf.shape(decoder_embedding_input)[0], tf.shape(decoder_embedding_input)[1], -1))\n",
    "          \n",
    "    decoder_Q_split = tf.split(decoder_Q,2,axis=2)\n",
    "    decoder_K_split = tf.split(decoder_K,2,axis=2)\n",
    "    decoder_V_split = tf.split(decoder_V,2,axis=2)\n",
    "    \n",
    "    decoder_Q_concat = tf.concat(decoder_Q_split,axis=0)\n",
    "    decoder_K_concat = tf.concat(decoder_K_split,axis=0)\n",
    "    decoder_V_concat = tf.concat(decoder_V_split,axis=0)\n",
    "    \n",
    "    decoder_sa_attention_map_raw = tf.matmul(decoder_Q_concat, tf.transpose(decoder_K_concat,[0,2,1]))\n",
    "    decoder_sa_attention_map = decoder_sa_attention_map_raw / 8\n",
    "    \n",
    "    '''\n",
    "        mask的作用是:\n",
    "            在decoder过程中,得到每个阶段的输出的时候,我们是看不到后面的信息的,\n",
    "            for example:\n",
    "                机 器 学 习 ---> machine learning\n",
    "                在输入machine的时候,无法看到learning的信息,因此在计算attention矩阵的时候[machine, learning]\n",
    "                的信息是没有的,所以要mask掉\n",
    "    ''' \n",
    "    print('before mask:', decoder_sa_attention_map)\n",
    "    \n",
    "    diag_vals = tf.ones_like(decoder_sa_attention_map[0, :, :])\n",
    "    print('diag_vals', diag_vals)\n",
    "    \n",
    "    tril = tf.linalg.LinearOperatorLowerTriangular(diag_vals).to_dense()\n",
    "    print('tril', tril)\n",
    "    \n",
    "    masks = tf.tile(tf.expand_dims(tril, 0), [tf.shape(decoder_sa_attention_map)[0], 1, 1])\n",
    "    print('masks', masks)\n",
    "    \n",
    "    paddings = tf.ones_like(masks) * (-2 ** 32 + 1)\n",
    "    print('paddings', paddings)\n",
    "    \n",
    "    decoder_sa_attention_map = tf.where(tf.equal(masks, 0), paddings, decoder_sa_attention_map)\n",
    "    print('decoder_sa_attention_map', decoder_sa_attention_map)\n",
    "    \n",
    "    decoder_sa_attention_map = tf.nn.softmax(decoder_sa_attention_map)\n",
    "\n",
    "    print('decoder_sa_attention_map:')\n",
    "    print(decoder_sa_attention_map)\n",
    "    "
   ],
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "decoder_sa_outputs:\ntf.Tensor(\n[[[1.0833601 2.1667202 3.25008   4.3334403]\n  [1.1666678 2.3333356 3.5000033 4.6666713]]\n\n [[1.22016   2.44032   3.66048   4.88064  ]\n  [1.1634135 2.326827  3.4902406 4.653654 ]]], shape=(2, 2, 4), dtype=float32)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "\n",
    "w_Z_decoder_sa = tf.constant([[0.1,0.2,0.3,0.4],\n",
    "                              [0.1,0.2,0.3,0.4],\n",
    "                              [0.1,0.2,0.3,0.4],\n",
    "                              [0.1,0.2,0.3,0.4],\n",
    "                              [0.1,0.2,0.3,0.4],\n",
    "                              [0.1,0.2,0.3,0.4]], dtype=tf.float32)\n",
    "\n",
    "weightedSumV = tf.matmul(decoder_sa_attention_map, decoder_V_concat)\n",
    "# print(weightedSumV)\n",
    "decoder_outputs_z = tf.concat(tf.split(weightedSumV, 2, axis=0), axis=2)\n",
    "# print(decoder_outputs_z)\n",
    "decoder_sa_outputs = tf.matmul(tf.reshape(decoder_outputs_z, (-1, tf.shape(decoder_outputs_z)[2])), w_Z_decoder_sa)\n",
    "\n",
    "decoder_sa_outputs = tf.reshape(decoder_sa_outputs, (tf.shape(decoder_embedding_input)[0], tf.shape(decoder_embedding_input)[1], -1))\n",
    "\n",
    "print('decoder_sa_outputs:')\n",
    "print(decoder_sa_outputs)\n"
   ],
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "encoder-decoder attention:\ndecoder_outputs:\ntf.Tensor(\n[[[32.96538  52.862305 83.22769  82.18768 ]\n  [34.217464 54.50911  85.57657  84.316574]]\n\n [[33.886017 53.9967   84.782715 83.54272 ]\n  [32.895317 52.705524 82.950836 81.89084 ]]], shape=(2, 2, 4), dtype=float32)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "\n",
    "print('encoder-decoder attention:')\n",
    "\n",
    "w_Q_decoder_sa2 = tf.constant([[0.2,0.3,0.4,0.5,0.6,0.7],\n",
    "                   [0.3,0.4,0.5,0.6,0.7,0.8],\n",
    "                   [0.4,0.5,0.6,0.7,0.8,0.9],\n",
    "                   [0.5,0.6,0.7,0.8,0.9,1]],dtype=tf.float32)\n",
    "\n",
    "w_K_decoder_sa2 = tf.constant([[0.18,0.28,0.38,0.48,0.58,0.68],\n",
    "                   [0.28,0.38,0.48,0.58,0.68,0.78],\n",
    "                   [0.38,0.48,0.58,0.68,0.78,0.88],\n",
    "                   [0.48,0.58,0.68,0.78,0.88,0.98]],dtype=tf.float32)\n",
    "\n",
    "w_V_decoder_sa2 = tf.constant([[0.22,0.32,0.42,0.52,0.62,0.72],\n",
    "                   [0.32,0.42,0.52,0.62,0.72,0.82],\n",
    "                   [0.42,0.52,0.62,0.72,0.82,0.92],\n",
    "                   [0.52,0.62,0.72,0.82,0.92,1.02]],dtype=tf.float32)\n",
    "\n",
    "w_Z_decoder_sa2 = tf.constant([[0.1,0.2,0.3,0.4],\n",
    "                   [0.1,0.2,0.3,0.4],\n",
    "                   [0.1,0.2,0.3,0.4],\n",
    "                   [0.1,0.2,0.3,0.4],\n",
    "                   [0.1,0.2,0.3,0.4],\n",
    "                   [0.1,0.2,0.3,0.4]],dtype=tf.float32)\n",
    "\n",
    "with tf.variable_scope('decoder_encoder_attention_block:'):\n",
    "    \n",
    "    decoder_sa_outputs += decoder_embedding_input\n",
    "    \n",
    "    encoder_decoder_Q = tf.matmul(tf.reshape(decoder_sa_outputs, (-1, tf.shape(decoder_sa_outputs)[2])), w_Q_decoder_sa2)\n",
    "    encoder_decoder_K = tf.matmul(tf.reshape(encoder_outputs, (-1, tf.shape(encoder_outputs)[2])), w_K_decoder_sa2)\n",
    "    encoder_decoder_V = tf.matmul(tf.reshape(encoder_outputs, (-1, tf.shape(encoder_outputs)[2])), w_V_decoder_sa2)\n",
    "    \n",
    "    encoder_decoder_Q = tf.reshape(encoder_decoder_Q, (tf.shape(decoder_embedding_input)[0], tf.shape(decoder_embedding_input)[1], -1))\n",
    "    encoder_decoder_K = tf.reshape(encoder_decoder_K, (tf.shape(encoder_outputs)[0], tf.shape(encoder_outputs)[1], -1))\n",
    "    encoder_decoder_V = tf.reshape(encoder_decoder_V, (tf.shape(encoder_outputs)[0], tf.shape(encoder_outputs)[1], -1))\n",
    "    \n",
    "    encoder_decoder_Q_split = tf.split(encoder_decoder_Q, 2, axis=2)\n",
    "    encoder_decoder_K_split = tf.split(encoder_decoder_K, 2, axis=2)\n",
    "    encoder_decoder_V_split = tf.split(encoder_decoder_V, 2, axis=2)\n",
    "    \n",
    "    encoder_decoder_Q_concat = tf.concat(encoder_decoder_Q_split, axis=0)\n",
    "    encoder_decoder_K_concat = tf.concat(encoder_decoder_K_split, axis=0)\n",
    "    encoder_decoder_V_concat = tf.concat(encoder_decoder_V_split, axis=0)\n",
    "    \n",
    "    encoder_decoder_attention_map_raw = tf.matmul(encoder_decoder_Q_concat, tf.transpose(encoder_decoder_K_concat, [0, 2, 1]))\n",
    "    encoder_decoder_attention_map_raw /= 8\n",
    "    encoder_decoder_attention_map = tf.nn.softmax(encoder_decoder_attention_map_raw)\n",
    "    \n",
    "    weightedSumV = tf.matmul(encoder_decoder_attention_map, encoder_decoder_V_concat)\n",
    "    \n",
    "    encoder_decoder_outputs_z = tf.concat(tf.split(weightedSumV, 2, axis=0), axis=2)\n",
    "    encoder_decoder_outputs = tf.matmul(tf.reshape(encoder_decoder_outputs_z, (-1, tf.shape(encoder_decoder_outputs_z)[2])), w_Z_decoder_sa2)\n",
    "    \n",
    "    encoder_decoder_attention_outputs = tf.reshape(encoder_decoder_outputs, (tf.shape(decoder_embedding_input)[0], tf.shape(decoder_embedding_input)[1], -1))\n",
    "    \n",
    "    encoder_decoder_attention_outputs += decoder_sa_outputs\n",
    "    # print(encoder_decoder_attention_outputs)\n",
    "    \n",
    "    W_f = tf.constant([[0.2,0.3,0.5,0.4],\n",
    "                       [0.2,0.3,0.5,0.4],\n",
    "                       [0.2,0.3,0.5,0.4],\n",
    "                       [0.2,0.3,0.5,0.4]])\n",
    "    \n",
    "    decoder_ffn_outputs = tf.matmul(tf.reshape(encoder_decoder_attention_outputs, (-1, tf.shape(encoder_decoder_attention_outputs)[2])), W_f)\n",
    "    decoder_ffn_outputs = tf.reshape(decoder_ffn_outputs, (tf.shape(encoder_decoder_attention_outputs)[0], tf.shape(encoder_decoder_attention_outputs)[1], -1))\n",
    "    \n",
    "    decoder_outputs = decoder_ffn_outputs + encoder_decoder_attention_outputs\n",
    "    \n",
    "    print('decoder_outputs:')\n",
    "    print(decoder_outputs)\n",
    "    "
   ],
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "tf.Tensor(\n[[[1.8448510e-33 1.5041991e-22 1.0000000e+00 1.2264673e-11]\n  [2.0177409e-34 3.4401375e-23 1.0000000e+00 5.8652909e-12]]\n\n [[4.1597021e-34 5.5724043e-23 1.0000000e+00 7.4648256e-12]\n  [2.3456182e-33 1.7653890e-22 1.0000000e+00 1.3286693e-11]]], shape=(2, 2, 4), dtype=float32)\ntf.Tensor(\n[[[0. 1. 0. 0.]\n  [0. 0. 1. 0.]]\n\n [[0. 0. 1. 0.]\n  [0. 1. 0. 0.]]], shape=(2, 2, 4), dtype=float32)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "\n",
    "W_final = tf.constant([[0.2,0.3,0.5,0.4],\n",
    "                       [0.2,0.3,0.5,0.4],\n",
    "                       [0.2,0.3,0.5,0.4],\n",
    "                       [0.2,0.3,0.5,0.4]])\n",
    "\n",
    "logits = tf.matmul(tf.reshape(decoder_outputs, (-1, tf.shape(decoder_outputs)[2])), W_final)\n",
    "logits = tf.reshape(logits, (tf.shape(decoder_outputs)[0], tf.shape(decoder_outputs)[1], -1))\n",
    "\n",
    "logits = tf.nn.softmax(logits)\n",
    "print(logits)\n",
    "\n",
    "y = tf.one_hot(decoder_input, depth=4)\n",
    "print(y)\n",
    "\n",
    "# loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y)\n",
    "# \n",
    "# train_op = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(loss)\n"
   ],
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": "\n",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}